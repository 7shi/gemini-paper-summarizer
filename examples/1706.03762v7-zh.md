# 标题

以下是对该论文的分析和总结：

**论文题目：** Attention Is All You Need

**摘要：**
本文提出了Transformer模型，这是一种完全基于注意力机制的新型网络架构，无需循环或卷积。在机器翻译任务上的实验表明，该模型在质量上优于现有模型，同时具有更高的并行性，并且训练时间显著减少。

**主要内容：**

1.  **模型架构：** Transformer模型采用编码器-解码器结构，由多层自注意力和前馈网络组成。
    *   **自注意力机制：** 模型使用缩放点积注意力，并通过多头注意力机制来捕捉不同表示子空间的信息。
    *   **位置编码：** 由于模型不包含循环或卷积，因此使用位置编码来注入序列顺序信息。

2.  **实验结果：**
    *   在WMT 2014英德翻译任务上，Transformer模型达到了28.4 BLEU，超过了现有最佳结果。
    *   在WMT 2014英法翻译任务上，该模型获得了41.8 BLEU的新单模型最佳BLEU分数。
    *   实验表明，Transformer模型可以很好地推广到其他任务，例如英语成分句法分析。

3.  **自注意力机制的优势：**
    *   **计算复杂度低：** 自注意力层的计算复杂度低于循环层，当序列长度小于表示维度时，自注意力层比循环层更快。
    *   **高度并行化：** 自注意力机制允许更多的并行计算，减少了训练时间。
    *   **路径长度短：** 自注意力层将所有位置连接起来，减少了网络中长距离依赖的路径长度。

**主要公式：**

*   **缩放点积注意力：**
    $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

*   **前馈网络：**
    $FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$

*   **学习率：**
    $lrate = d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})$

**结论：**
Transformer模型是一种强大的新型序列转导模型，它完全基于注意力机制，无需循环或卷积。该模型在机器翻译任务上取得了显著的成果，并且具有高度并行化和可推广性。

**中文翻译：**

**题目：** 注意力机制就是你所需要的

# Abstract

本文提出了一种新的简单的网络架构，即 Transformer，它完全基于注意力机制，摒弃了循环和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优越，同时具有更高的并行性，并且训练所需的时间显著减少。我们的模型在 WMT 2014 英德翻译任务上达到了 28.4 BLEU，比现有的最佳结果（包括集成模型）提高了 2 个以上的 BLEU。在 WMT 2014 英法翻译任务上，我们的模型在 8 个 GPU 上训练 3.5 天后，建立了一个新的单模型最先进的 BLEU 分数 41.8，这只是文献中最佳模型训练成本的一小部分。我们表明，通过将其成功地应用于具有大量和有限训练数据的英语成分句法分析，Transformer 可以很好地推广到其他任务。

# 概述

本文提出了一种名为Transformer的新型神经网络架构，它完全基于注意力机制，摒弃了循环和卷积，并在机器翻译任务上取得了优异的性能，同时具有更高的并行性和更低的训练时间。

## 问题陈述

这篇论文试图解决序列转导模型中对循环神经网络（RNN）或卷积神经网络（CNN）的依赖问题。这些模型在处理序列数据时，通常包含编码器和解码器，并通过注意力机制连接。然而，循环神经网络的固有顺序计算特性限制了并行化能力，尤其是在处理长序列时。卷积神经网络虽然可以并行计算，但其关联远距离位置的能力会随着距离的增加而减弱。

因此，这篇论文提出了一个名为Transformer的全新网络架构，它完全依赖于注意力机制，摒弃了循环和卷积操作。Transformer旨在通过注意力机制直接捕捉输入和输出序列之间的全局依赖关系，从而提高并行化能力，并减少训练时间，同时保持甚至提高翻译质量。此外，论文还探讨了Transformer在其他任务上的泛化能力，如英语成分句法分析。

## 方法论

该论文提出了一种名为 Transformer 的新型神经网络架构，用于序列转换任务，例如机器翻译。该架构完全基于注意力机制，摒弃了传统的循环神经网络 (RNN) 和卷积神经网络 (CNN)。

具体来说，Transformer 架构包含以下关键组成部分：

1.  **编码器-解码器结构**: 类似于传统的序列转换模型，Transformer 采用编码器-解码器结构。编码器将输入序列映射为连续的表示，解码器则基于这些表示生成输出序列。
2.  **多头自注意力机制**: Transformer 的核心是多头自注意力机制，它允许模型在计算每个位置的表示时，同时关注输入序列中的所有位置。这种机制可以有效地捕捉长距离依赖关系，且易于并行化。
3.  **位置编码**: 由于 Transformer 不使用循环或卷积，需要引入位置编码来表示序列中 token 的顺序信息。论文中使用正弦和余弦函数生成位置编码。
4.  **前馈网络**: 除了注意力子层之外，编码器和解码器的每一层都包含一个位置无关的前馈网络，用于进一步处理表示。
5.  **残差连接和层归一化**: 为了方便训练，模型在每个子层周围使用残差连接，并进行层归一化。

该论文的主要贡献在于：

*   提出了一种完全基于注意力机制的序列转换模型，无需循环或卷积。
*   证明了 Transformer 在机器翻译任务上可以达到最先进的性能，同时训练时间显著减少。
*   展示了 Transformer 的泛化能力，可以应用于其他任务，如英语成分句法分析。

总而言之，该论文提出了一种新的模型架构，并证明了其在序列转换任务上的有效性和效率，为后续研究奠定了基础。

## 创新性

本文的主要创新点在于提出了 Transformer 模型，这是一个完全基于注意力机制的序列转换模型，摒弃了传统的循环神经网络（RNN）和卷积神经网络（CNN）。

具体来说，创新点包括：

1.  **完全基于注意力机制：** Transformer 模型完全依赖于注意力机制来捕捉输入和输出序列之间的全局依赖关系，而不再使用 RNN 的循环结构或 CNN 的卷积操作。这使得模型可以并行处理序列中的所有位置，显著提高了训练速度。
2.  **多头注意力机制：** Transformer 模型引入了多头注意力机制，允许模型从不同的表示子空间中联合关注信息。这种机制增强了模型的表达能力，并能捕捉到更丰富的语义信息。
3.  **位置编码：** 由于 Transformer 模型不具备序列的顺序感知能力，论文提出了位置编码的概念，将位置信息添加到输入嵌入中，使模型能够感知序列中不同位置的 tokens。
4.  **更快的训练速度：** 由于 Transformer 模型可以并行处理序列，其训练速度远快于传统的 RNN 模型。这使得在较短的时间内训练出更高质量的模型成为可能。
5.  **在机器翻译任务上取得卓越性能：** 实验结果表明，Transformer 模型在英语到德语和英语到法语的机器翻译任务上都取得了当时的最先进水平，并且训练成本远低于其他模型。
6.  **良好的泛化能力：** Transformer 模型不仅在机器翻译任务上表现出色，而且在英语成分句法分析等其他任务上也展现了良好的泛化能力。

总而言之，Transformer 模型的创新之处在于其完全基于注意力机制的架构，这使得模型在训练速度、性能和泛化能力方面都取得了显著的提升，为后续的自然语言处理模型的发展奠定了基础。

# 文档结构

- 1 Introduction
- 2 Background
- 3 Model Architecture
  - 3.1 Encoder and Decoder Stacks
  - 3.2 Attention
    - 3.2.1 Scaled Dot-Product Attention
    - 3.2.2 Multi-Head Attention
    - 3.2.3 Applications of Attention in our Model
  - 3.3 Position-wise Feed-Forward Networks
  - 3.4 Embeddings and Softmax
  - 3.5 Positional Encoding
- 4 Why Self-Attention
- 5 Training
  - 5.1 Training Data and Batching
  - 5.2 Hardware and Schedule
  - 5.3 Optimizer
  - 5.4 Regularization
- 6 Results
  - 6.1 Machine Translation
  - 6.2 Model Variations
  - 6.3 English Constituency Parsing
- 7 Conclusion
- Acknowledgements
- References
- Attention Visualizations

## 1 Introduction

本文提出了一种名为 Transformer 的新型网络架构，用于序列转导任务，该架构完全基于注意力机制，摒弃了循环和卷积操作。循环神经网络（RNN）及其变体，如长短期记忆网络（LSTM）和门控循环单元（GRU），在序列建模和转导问题中占据主导地位。然而，循环模型的固有顺序性限制了训练过程中的并行化，尤其是在处理长序列时。注意力机制在序列建模和转导模型中变得至关重要，它允许模型在不考虑输入或输出序列中距离的情况下建模依赖关系。尽管如此，大多数注意力机制仍然与循环网络结合使用。本文提出的 Transformer 模型旨在通过完全依赖注意力机制来解决这些问题，从而实现更高的并行化并达到新的翻译质量水平。

## 2 Background

第二节“背景”主要讨论了减少序列计算的目标，并以此引出了扩展神经GPU、ByteNet和ConvS2S等模型。这些模型都使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。然而，这些模型中，任意两个输入或输出位置之间的信号关联所需的操作数量会随着位置之间的距离而增长，ConvS2S呈线性增长，而ByteNet呈对数增长，这使得学习远距离位置之间的依赖关系更加困难。Transformer模型将此操作数量减少为常数，但代价是由于平均注意力加权位置而降低了有效分辨率。为了弥补这一缺陷，Transformer采用了多头注意力机制。

此外，该节还提到了自注意力机制，它通过关联单个序列的不同位置来计算序列的表示，并已成功应用于多种任务。最后，该节指出，Transformer是第一个完全依赖自注意力机制来计算输入和输出表示的转换模型，而没有使用序列对齐的RNN或卷积。

## 3 Model Architecture

大多数具有竞争力的神经序列转换模型采用编码器-解码器结构。编码器将输入符号序列 $(x_1,..., x_n)$ 映射为连续表示序列 $z = (z_1, ..., z_n)$。解码器则根据 $z$ 逐个生成输出符号序列 $(y_1, ..., y_m)$。模型在每一步都是自回归的，即在生成下一个符号时，会把之前生成的符号作为额外输入。

## 3.1 编码器和解码器堆栈

**编码器:** 编码器由 $N=6$ 个相同的层堆叠而成。每层包含两个子层：一个多头自注意力机制和一个简单的逐位置全连接前馈网络。每个子层都使用残差连接和层归一化，即子层的输出为 $\text{LayerNorm}(x + \text{Sublayer}(x))$。为了方便残差连接，模型的所有子层以及嵌入层都产生维度为 $d_{model} = 512$ 的输出。

**解码器:** 解码器也由 $N=6$ 个相同的层堆叠而成。除了编码器层中的两个子层外，解码器还插入了第三个子层，该子层对编码器堆栈的输出执行多头注意力。与编码器类似，解码器也使用残差连接和层归一化。为了防止解码器中的位置关注后续位置，解码器的自注意力子层进行了修改，通过掩码操作确保位置 $i$ 的预测仅依赖于位置小于 $i$ 的已知输出。

## 3.2 注意力机制

注意力函数将查询（query）和一组键-值对（key-value pairs）映射到输出，其中查询、键、值和输出都是向量。输出计算为值的加权和，其中每个值的权重由查询与对应键的兼容性函数计算得出。

### 3.2.1 缩放点积注意力

本文提出的注意力机制称为“缩放点积注意力”。输入包括维度为 $d _ k$ 的查询和键，以及维度为 $d _ v$ 的值。查询与所有键进行点积运算，然后除以 $\sqrt{d _ k}$，最后应用 softmax 函数获得值的权重。在实践中，注意力函数同时对一组查询进行计算，将查询、键和值分别打包成矩阵 $Q$, $K$ 和 $V$ 。输出矩阵计算如下：

$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$

### 3.2.2 多头注意力

多头注意力不是使用 $d_{model}$ 维的键、值和查询执行单个注意力函数，而是将查询、键和值线性投影 $h$ 次到不同的维度 $d _ k$, $d _ k$ 和 $d _ v$，然后并行执行注意力函数，得到 $d _ v$ 维的输出。这些输出被拼接并再次投影，得到最终值。多头注意力允许模型联合关注不同表示子空间的不同位置信息。

$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head} _ 1, ..., \text{head} _ h)W^O$

其中 $\text{head} _ i = \text{Attention}(QW _ i^Q, KW _ i^K, VW _ i^V)$，投影矩阵为 $W _ i^Q \in \mathbb{R}^{d _ {model} \times d _ k}$, $W _ i^K \in \mathbb{R}^{d _ {model} \times d _ k}$, $W _ i^V \in \mathbb{R}^{d _ {model} \times d _ v}$, $W^O \in \mathbb{R}^{hd _ v \times d _ {model}}$ 。

本文使用 $h=8$ 个并行注意力层，每个头的维度为 $d _ k = d _ v = d _ {model}/h = 64$ 。

### 3.2.3 模型中注意力的应用

Transformer 模型在三种不同的方式中使用多头注意力：

*   **编码器-解码器注意力层:** 查询来自之前的解码器层，键和值来自编码器的输出，允许解码器的每个位置关注输入序列的所有位置。
*   **编码器自注意力层:** 所有的键、值和查询都来自编码器前一层的输出，允许编码器的每个位置关注编码器前一层的所有位置。
*   **解码器自注意力层:** 允许解码器的每个位置关注解码器中所有之前的位置。为了保持自回归特性，解码器中的缩放点积注意力通过掩码操作防止左侧信息流。

## 3.3 逐位置前馈网络

除了注意力子层之外，编码器和解码器的每一层都包含一个逐位置全连接前馈网络，该网络独立且相同地应用于每个位置。该网络由两个线性变换组成，中间有一个 ReLU 激活函数：

$\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2$

输入和输出维度为 $d_{model} = 512$，中间层的维度为 $d_{ff} = 2048$。

## 3.4 嵌入和 Softmax

与其它序列转换模型类似，本文使用学习到的嵌入将输入和输出标记转换为 $d_{model}$ 维的向量。解码器输出通过线性变换和 softmax 函数转换为预测的下一个标记的概率。嵌入层和预 softmax 线性变换共享相同的权重矩阵，嵌入层的权重乘以 $\sqrt{d_{model}}$。

## 3.5 位置编码

由于模型不包含循环和卷积，为了使模型利用序列的顺序信息，需要在输入嵌入中加入位置编码。位置编码与嵌入具有相同的维度 $d_{model}$, 可以进行求和。本文使用不同频率的正弦和余弦函数：

$PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{model}})$

$PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{model}})$

其中 $pos$ 是位置, $i$ 是维度。这种选择是因为它允许模型通过相对位置进行学习，因为对于任何固定的偏移 $k$, $PE _ {pos+k}$ 可以表示为 $PE _ {pos}$ 的线性函数。

## 4 Why Self-Attention

第四节“为何选择自注意力机制”探讨了自注意力层相对于循环层和卷积层在序列建模中的优势，并提出了三个主要考量因素：

1. **每层的计算复杂度**: 自注意力层的计算复杂度为 $O(n^2 \cdot d)$, 其中 $n$ 是序列长度, $d$ 是表示维度。循环层的计算复杂度为 $O(n \cdot d^2)$, 而卷积层的计算复杂度为 $O(k \cdot n \cdot d^2)$, 其中 $k$ 是卷积核大小。当序列长度 $n$ 小于表示维度 $d$ 时，自注意力层的计算速度比循环层更快。

2. **并行化程度**: 自注意力层可以在序列的所有位置上并行计算，只需要 $O(1)$ 的顺序操作。循环层需要 $O(n)$ 的顺序操作，而卷积层需要 $O(1)$ 或 $O(log _ k(n))$ 的顺序操作。因此，自注意力层更易于并行化。

3. **长距离依赖路径长度**: 自注意力层通过一个常数数量的顺序操作连接所有位置，而循环层需要 $O(n)$ 的路径长度，卷积层需要 $O(n/k)$ 或 $O(log _ k(n))$ 的路径长度。自注意力层具有最短的路径长度，更容易学习长距离依赖关系。

此外，自注意力机制还具有更高的可解释性，可以通过可视化注意力分布来分析模型学习到的句法和语义结构。

## 5 Training

本章描述了模型训练的方案。

**5.1 训练数据与批处理**

*   **数据集:** 使用了WMT 2014英德数据集，包含约450万个句子对，采用字节对编码（BPE），源-目标词汇共享，大小约为37000。对于英法翻译，使用了更大的WMT 2014英法数据集，包含3600万个句子，词汇大小为32000。
*   **批处理:** 句子对按近似序列长度进行批处理，每个训练批次包含大约25000个源语言token和25000个目标语言token。

**5.2 硬件与时间安排**

*   **硬件:** 模型在一台配备8个NVIDIA P100 GPU的机器上进行训练。
*   **训练时间:** 基础模型每步训练耗时约0.4秒，总共训练10万步（约12小时）。大型模型每步训练耗时约1.0秒，总共训练30万步（3.5天）。

**5.3 优化器**

*   **优化器:** 使用Adam优化器，参数为 $\beta _ 1=0.9$, $\beta _ 2=0.98$, $\epsilon=10^{-9}$ 。
*   **学习率调整:** 学习率在训练过程中动态变化，公式如下：  
    $lrate = d _ {model}^{-0.5} \cdot min(step\\_num^{-0.5}, step\\_num \cdot warmup\\_steps^{-1.5})$  
    其中, $warmup\\_steps = 4000$ 。学习率在前 $warmup\\_steps$ 步线性增加，之后按步数平方根的倒数比例递减。

**5.4 正则化**

训练过程中采用了三种正则化方法：

*   **残差Dropout:** 在每个子层输出后，加入残差连接前使用dropout，同时对编码器和解码器堆栈中的嵌入和位置编码之和也应用dropout。基础模型dropout率为0.1。
*   **标签平滑:** 训练过程中使用标签平滑，值为 $\epsilon _ {ls} = 0.1$ 。这会降低困惑度，但能提高准确率和BLEU得分。

## 6 Results

本节展示了Transformer模型在机器翻译和英语成分句法分析任务上的实验结果，并分析了模型不同变体的影响。

### 6.1 机器翻译

在WMT 2014英德翻译任务上，大型Transformer模型取得了28.4的BLEU值，超越了之前所有报道的最佳模型（包括集成模型），创造了新的技术水平。该模型的训练使用了8个P100 GPU，耗时3.5天。即使是基础Transformer模型，也以远低于其他竞争模型的训练成本，超越了所有之前发表的模型和集成模型。

在WMT 2014英法翻译任务上，大型Transformer模型取得了41.0的BLEU值，超过了之前所有发表的单模型，且训练成本不到之前最佳模型的1/4。用于英法翻译的大型Transformer模型使用了0.1的dropout率，而不是0.3。

对于基础模型，使用了最后5个检查点的平均值，这些检查点以10分钟的间隔写入。对于大型模型，使用了最后20个检查点的平均值。使用了束搜索，束大小为4，长度惩罚α=0.6。这些超参数是在开发集上实验后选择的。将推理期间的最大输出长度设置为输入长度+50，但尽可能提前终止。

表2总结了结果，并将翻译质量和训练成本与其他模型的架构进行了比较。通过将训练时间、使用的GPU数量以及每个GPU的持续单精度浮点容量的估计值相乘，估计了用于训练模型的浮点运算次数。

### 6.2 模型变体

为了评估Transformer不同组成部分的重要性，研究者以不同的方式改变了基础模型，并测量了在英德翻译任务开发集newstest2013上的性能变化。使用了上一节中描述的束搜索，但没有使用检查点平均。表3中展示了这些结果。

表3的(A)行中，研究者改变了注意力头的数量以及注意力键和值的维度，保持计算量不变。结果表明，单头注意力比最佳设置差0.9个BLEU值，且注意力头过多也会降低质量。

表3的(B)行中，观察到减小注意力键大小 $d _ k$ 会损害模型质量。这表明确定兼容性并非易事，且点积之外的更复杂的兼容性函数可能是有益的。

表3的(C)和(D)行中，正如预期的那样，更大的模型更好，并且dropout对于避免过拟合非常有帮助。

表3的(E)行中，研究者用学习的位置嵌入替换了正弦位置编码，并观察到与基础模型几乎相同的结果。

### 6.3 英语成分句法分析

为了评估Transformer是否可以推广到其他任务，研究者在英语成分句法分析上进行了实验。该任务提出了特定的挑战：输出受到很强的结构约束，并且比输入长得多。此外，RNN序列到序列模型在这种小数据机制下未能达到最先进的结果。

研究者在Penn Treebank的华尔街日报（WSJ）部分（约40K个训练句子）上训练了一个具有 $d _ {model}=1024$ 的4层Transformer。还在半监督设置中进行了训练，使用了更大的高置信度BerkeleyParser语料库，其中包含约17M个句子。对于仅WSJ设置，使用了16K个token的词汇表，对于半监督设置，使用了32K个token的词汇表。

研究者只进行了少量的实验来选择dropout、注意力、残差（第5.4节）、学习率和第22节开发集上的束大小，所有其他参数都与英德翻译基础模型保持不变。在推理过程中，将最大输出长度增加到输入长度+300。对于仅WSJ和半监督设置，都使用了21的束大小和α=0.3。

表4的结果表明，尽管缺乏特定于任务的调整，模型表现出惊人的良好性能，产生了比之前所有报道的模型更好的结果，除了循环神经网络语法（RNNG）。

与RNN序列到序列模型不同，即使只在40K个句子的WSJ训练集上训练，Transformer也优于BerkeleyParser。

## 7 Conclusion

本文提出了 Transformer 模型，这是一个完全基于注意力机制的序列转换模型，它用多头自注意力取代了编码器-解码器架构中最常用的循环层。

对于翻译任务，Transformer 的训练速度明显快于基于循环或卷积层的架构。在 WMT 2014 英德和 WMT 2014 英法翻译任务中，该模型都达到了新的最先进水平。在前一个任务中，该模型甚至优于所有先前报告的集成模型。

作者对基于注意力的模型的未来感到兴奋，并计划将其应用于其他任务。他们计划将 Transformer 扩展到文本以外的输入和输出模式的问题，并研究局部、受限的注意力机制，以有效地处理大型输入和输出，例如图像、音频和视频。使生成过程减少顺序性是作者的另一个研究目标。

## Acknowledgements

作者感谢Nal Kalchbrenner和Stephan Gouws的有益评论、更正和启发。

## References

此部分列出了本文档中引用的所有参考文献，共计40篇。这些参考文献涵盖了以下几个主要领域：

1.  **神经网络和深度学习**: 包括层归一化、循环神经网络（RNN）、长短期记忆网络（LSTM）、门控循环单元（GRU）、深度残差学习、dropout正则化、以及各种优化算法（如Adam）。
    *   例如，\[1] 介绍了层归一化，\[7] 评估了门控循环神经网络，\[11] 提出了深度残差学习，\[33] 探讨了dropout正则化，\[20] 介绍了Adam优化器。

2.  **序列建模和机器翻译**: 包括基于RNN的编码器-解码器模型、注意力机制、卷积序列到序列模型、神经机器翻译等。
    *   例如，\[2] 提出了基于注意力机制的神经机器翻译，\[5] 介绍了RNN编码器-解码器模型，\[9] 提出了卷积序列到序列学习，\[35] 描述了序列到序列学习。

3.  **自然语言处理和句法分析**: 包括自注意力机制、成分句法分析、以及各种语法模型。
    *   例如，\[4] 介绍了用于机器阅读的LSTM网络，\[8] 提出了循环神经网络语法，\[14] 提出了自训练PCFG语法，\[22] 提出了结构化的自注意力句子嵌入，\[25] 介绍了Penn Treebank，\[29] 介绍了学习精确的树形标注，\[40] 介绍了快速准确的移位归约成分句法分析。

4.  **其他相关技术**: 包括词嵌入、子词单元、稀疏门控混合专家层、以及各种模型压缩技术。
    *   例如，\[30] 提出了利用输出嵌入改进语言模型，\[31] 介绍了使用子词单元进行神经机器翻译，\[32] 提出了稀疏门控混合专家层，\[21] 介绍了LSTM网络的分解技巧。

这些参考文献为本文提出的Transformer模型提供了理论基础和技术背景，并展示了该模型在机器翻译和句法分析等任务上的优越性。

## Attention Visualizations

文章的“注意力可视化”部分展示了Transformer模型中注意力机制如何运作的例子。

图3展示了编码器自注意力层中，第5层第6个头如何处理长距离依赖关系。例如，当模型处理“making”这个词时，它会注意到句子中较远位置的“difficult”，从而补全短语“making...more difficult”。图中不同颜色代表不同的注意力头。

图4展示了第5层第6个头在处理指代消解时的行为。上面部分显示了完整的注意力，下面部分则特别突出了“its”这个词的注意力。

图5展示了两个不同的注意力头，它们似乎与句子的结构有关。这些例子表明，编码器的自注意力层中的不同注意力头学习执行不同的任务。
