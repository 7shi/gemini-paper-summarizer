# Abstract

Les modèles dominants de transduction de séquences sont basés sur des réseaux neuronaux récurrents ou convolutionnels complexes qui comprennent un encodeur et un décodeur. Les modèles les plus performants connectent également l'encodeur et le décodeur via un mécanisme d'attention. Nous proposons une nouvelle architecture de réseau simple, le Transformer, basée uniquement sur des mécanismes d'attention, se dispensant entièrement de récurrence et de convolutions. Des expériences sur deux tâches de traduction automatique montrent que ces modèles sont de qualité supérieure tout en étant plus parallélisables et en nécessitant un temps d'entraînement considérablement plus court. Notre modèle atteint un score BLEU de 28,4 sur la tâche de traduction anglais-allemand WMT 2014, améliorant les meilleurs résultats existants, y compris les ensembles, de plus de 2 BLEU. Sur la tâche de traduction anglais-français WMT 2014, notre modèle établit un nouveau score BLEU de pointe de 41,8 après un entraînement de 3,5 jours sur huit GPU, une petite fraction des coûts d'entraînement des meilleurs modèles de la littérature. Nous montrons que le Transformer se généralise bien à d'autres tâches en l'appliquant avec succès à l'analyse syntaxique anglaise, à la fois avec des données d'entraînement importantes et limitées.

# Résumé

Ce document introduit le Transformer, une nouvelle architecture de réseau neuronal pour la transduction de séquences basée uniquement sur des mécanismes d'attention, surpassant les modèles récurrents et convolutifs existants en traduction automatique tout en étant plus parallélisable et rapide à entraîner.

## Énoncé du Problème

Le document cherche à résoudre les limitations des modèles de transduction de séquences dominants, qui sont basés sur des réseaux neuronaux récurrents ou convolutionnels complexes. Ces modèles présentent des difficultés en termes de parallélisation et de temps d'entraînement, en particulier pour les séquences longues. L'article propose une nouvelle architecture, le Transformer, qui se base uniquement sur des mécanismes d'attention, éliminant ainsi la récurrence et les convolutions, afin d'améliorer la parallélisation et de réduire le temps d'entraînement tout en maintenant, voire en améliorant, la qualité des résultats.

## Méthodologie

Le document propose une nouvelle architecture de réseau neuronal, appelée le "Transformer", pour la transduction de séquences. Cette architecture repose uniquement sur des mécanismes d'attention, en se dispensant des récurrences et des convolutions habituellement utilisées dans les modèles de transduction de séquences.

Plus précisément, le Transformer utilise :

*   **L'auto-attention multi-têtes** :  Le modèle calcule des représentations en mettant en relation les différentes positions d'une même séquence. Plusieurs têtes d'attention sont utilisées en parallèle pour capturer différentes relations.  
*   **Des connexions résiduelles et une normalisation des couches** : Ces techniques sont utilisées pour faciliter l'entraînement des réseaux profonds.  
*   **Un réseau de neurones à propagation avant positionnel** :  Un réseau de neurones à deux couches est appliqué à chaque position de la séquence, indépendamment des autres positions.
*   **Un encodage positionnel** :  Des fonctions sinusoïdales sont utilisées pour encoder la position des tokens dans la séquence, car le Transformer n'a pas de notion inhérente de l'ordre des séquences.

Le modèle est appliqué à des tâches de traduction automatique, en utilisant des données d'entraînement massives. Les résultats obtenus sont comparés avec ceux des modèles de l'état de l'art, en termes de qualité de traduction et de coût d'entraînement. Le document évalue également la généralisation du modèle en l'appliquant à une tâche d'analyse syntaxique.

En résumé, la méthodologie du document est centrée sur la proposition, la mise en œuvre et l'évaluation d'une nouvelle architecture de réseau neuronal, le Transformer, basée sur l'attention, pour des tâches de transduction de séquences.

## Nouveauté

Le document présente une nouvelle architecture de réseau neuronal appelée "Transformer", qui repose uniquement sur des mécanismes d'attention, en se dispensant de récurrence et de convolutions. Les expériences menées sur deux tâches de traduction automatique montrent que ces modèles sont supérieurs en termes de qualité, tout en étant plus parallélisables et en nécessitant un temps d'entraînement nettement inférieur. Le modèle atteint un score BLEU de 28,4 sur la tâche de traduction anglais-allemand WMT 2014, améliorant les meilleurs résultats existants, y compris les ensembles, de plus de 2 BLEU. Sur la tâche de traduction anglais-français WMT 2014, le modèle établit un nouveau score BLEU de pointe de 41,8 après un entraînement de 3,5 jours sur huit GPU. Le Transformer se généralise bien à d'autres tâches, comme le montre son application réussie à l'analyse syntaxique en anglais, avec des données d'entraînement importantes et limitées.

# Structure du Document

- 1 Introduction
- 2 Background
- 3 Model Architecture
  - 3.1 Encoder and Decoder Stacks
  - 3.2 Attention
    - 3.2.1 Scaled Dot-Product Attention
    - 3.2.2 Multi-Head Attention
    - 3.2.3 Applications of Attention in our Model
  - 3.3 Position-wise Feed-Forward Networks
  - 3.4 Embeddings and Softmax
  - 3.5 Positional Encoding
- 4 Why Self-Attention
- 5 Training
  - 5.1 Training Data and Batching
  - 5.2 Hardware and Schedule
  - 5.3 Optimizer
  - 5.4 Regularization
- 6 Results
  - 6.1 Machine Translation
  - 6.2 Model Variations
  - 6.3 English Constituency Parsing
- 7 Conclusion
- Acknowledgements
- References
- Attention Visualizations

## 1 Introduction

Les modèles de transduction de séquences dominants sont basés sur des réseaux neuronaux récurrents ou convolutionnels complexes, comprenant un encodeur et un décodeur. Les meilleurs modèles connectent également l'encodeur et le décodeur via un mécanisme d'attention. Les modèles récurrents calculent généralement le long des positions des symboles des séquences d'entrée et de sortie, générant une séquence d'états cachés $h_t$ en fonction de l'état caché précédent $h_{t-1}$ et de l'entrée à la position $t$. Cette nature séquentielle empêche la parallélisation lors de l'entraînement, ce qui devient critique pour les séquences plus longues. Bien que des améliorations de l'efficacité computationnelle aient été réalisées, la contrainte fondamentale du calcul séquentiel demeure. Les mécanismes d'attention sont devenus essentiels pour modéliser les dépendances sans égard à leur distance dans les séquences d'entrée ou de sortie, mais sont généralement utilisés conjointement avec un réseau récurrent. Cet article propose le Transformer, une architecture de modèle qui évite la récurrence et repose entièrement sur un mécanisme d'attention pour établir des dépendances globales entre l'entrée et la sortie. Le Transformer permet une parallélisation accrue et atteint une qualité de traduction de pointe après un entraînement de seulement douze heures sur huit GPU P100.

## 2 Background

La section 2, intitulée "Background", expose que l'objectif de réduire le calcul séquentiel est à la base des modèles Extended Neural GPU, ByteNet et ConvS2S, qui utilisent des réseaux neuronaux convolutionnels pour calculer en parallèle les représentations cachées. Le nombre d'opérations pour relier deux positions arbitraires croît linéairement (ConvS2S) ou logarithmiquement (ByteNet) avec la distance entre ces positions, ce qui rend difficile l'apprentissage des dépendances distantes. Le Transformer réduit ce nombre à une constante, au prix d'une résolution effective réduite, compensée par l'attention multi-tête. L'auto-attention, ou intra-attention, calcule une représentation d'une séquence en reliant ses différentes positions. Elle a été utilisée avec succès dans diverses tâches. Les réseaux de mémoire "end-to-end" utilisent un mécanisme d'attention récurrent au lieu d'une récurrence alignée sur la séquence. Le Transformer est le premier modèle de transduction utilisant uniquement l'auto-attention pour calculer les représentations d'entrée et de sortie, sans RNN ou convolution.

## 3 Model Architecture

Le modèle Transformer, une architecture de transduction de séquences, repose sur des mécanismes d'attention et des couches de connexion directe, évitant ainsi les réseaux récurrents et convolutionnels.

L'encodeur est constitué de $N=6$ couches identiques, chacune ayant deux sous-couches : une d'auto-attention multi-tête et une de réseau de neurones à connexion directe. Des connexions résiduelles et une normalisation de couche sont appliquées. Toutes les sous-couches et les couches d'intégration produisent des sorties de dimension $d_{\text{model}}=512$.

Le décodeur possède une structure similaire, avec une sous-couche d'attention supplémentaire sur la sortie de l'encodeur. L'auto-attention du décodeur est masquée pour empêcher l'attention sur les positions futures, assurant ainsi la propriété auto-régressive du modèle.

La fonction d'attention mappe une requête et un ensemble de paires clé-valeur vers une sortie, calculée comme une somme pondérée des valeurs. L'attention "Scaled Dot-Product" calcule les produits scalaires de la requête avec toutes les clés, en divisant par $\sqrt{d_k}$ avant d'appliquer une fonction softmax. Mathématiquement, l'attention est définie par :

$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$

L'attention multi-tête projette linéairement les requêtes, clés et valeurs $h$ fois, effectue l'attention sur chaque projection, et concatène les sorties. Cela permet au modèle de considérer l'information de différents sous-espaces de représentation.  Les projections sont définies par :

$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$
où
$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

Le modèle utilise l'attention multi-tête de trois manières : dans les couches "encodeur-décodeur" où les requêtes viennent du décodeur et les clés/valeurs de l'encodeur ; dans l'auto-attention de l'encodeur où les requêtes, clés et valeurs proviennent de la sortie de la couche précédente ; et dans l'auto-attention du décodeur avec un masquage pour préserver l'auto-régressivité.

Chaque couche de l'encodeur et du décodeur contient un réseau de neurones à connexion directe appliqué séparément à chaque position, composé de deux transformations linéaires avec une activation ReLU entre les deux :

$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$

Les entrées et sorties sont converties en vecteurs de dimension $d_{\text{model}}$ par des intégrations apprises. Une transformation linéaire et une fonction softmax sont utilisées pour prédire les probabilités du prochain jeton. Les poids des couches d'intégration et de la transformation linéaire pré-softmax sont partagés et multipliés par $\sqrt{d_{\text{model}}}$.

Enfin, des encodages positionnels sont ajoutés aux intégrations d'entrée pour permettre au modèle de tenir compte de l'ordre des jetons, étant donné l'absence de récurrence ou de convolution. Ces encodages sont définis par :

$
\text{PE}_{(\text{pos}, 2i)} = \sin(\text{pos}/10000^{2i/d_{\text{model}}})
$
$
\text{PE}_{(\text{pos}, 2i+1)} = \cos(\text{pos}/10000^{2i/d_{\text{model}}})
$

## 4 Why Self-Attention

La section 4, intitulée "Pourquoi l'auto-attention", compare les couches d'auto-attention avec les couches récurrentes et convolutionnelles, couramment utilisées pour transformer une séquence de représentations symboliques de longueur variable $(x_1, ..., x_n)$ en une autre séquence de même longueur $(z_1, ..., z_n)$, avec $x_i, z_i \in \mathbb{R}^d$. Trois critères sont utilisés pour justifier l'utilisation de l'auto-attention :

1.  **Complexité computationnelle par couche** : L'auto-attention connecte toutes les positions avec un nombre constant d'opérations séquentielles, tandis qu'une couche récurrente nécessite $O(n)$ opérations séquentielles. L'auto-attention est plus rapide que les couches récurrentes lorsque la longueur de la séquence $n$ est inférieure à la dimension de la représentation $d$. Les couches convolutionnelles nécessitent $O(n/k)$ couches avec des noyaux contigus, ou $O(\log_k(n))$ couches avec des convolutions dilatées, où $k$ est la taille du noyau.

2.  **Parallélisation** : L'auto-attention permet une parallélisation maximale, contrairement aux couches récurrentes qui nécessitent $O(n)$ opérations séquentielles.

3.  **Longueur du chemin** : L'auto-attention connecte toutes les positions avec un nombre constant d'opérations, ce qui facilite l'apprentissage des dépendances à longue portée. Les couches convolutionnelles nécessitent un nombre de couches qui dépend de la longueur de la séquence, ce qui rend plus difficile l'apprentissage des dépendances à longue portée.

En résumé, l'auto-attention permet une complexité computationnelle moindre, une parallélisation accrue et des chemins plus courts pour l'apprentissage des dépendances à longue portée, ce qui en fait un choix avantageux par rapport aux couches récurrentes et convolutionnelles.

## 5 Training

**5. Entraînement**

Cette section décrit le régime d'entraînement des modèles.

**5.1 Données d'entraînement et Batching**

Les modèles ont été entraînés sur le jeu de données standard WMT 2014 Anglais-Allemand, constitué d'environ 4,5 millions de paires de phrases. Les phrases ont été encodées en utilisant l'encodage par paires d'octets (byte-pair encoding), avec un vocabulaire source-cible partagé d'environ 37 000 jetons. Pour l'Anglais-Français, le jeu de données WMT 2014 Anglais-Français, plus grand, a été utilisé, constitué de 36 millions de phrases, avec un vocabulaire de 32 000 word-pieces. Les paires de phrases ont été regroupées en lots (batch) en fonction de la longueur approximative de la séquence. Chaque lot d'entraînement contenait un ensemble de paires de phrases contenant approximativement 25 000 jetons sources et 25 000 jetons cibles.

**5.2 Matériel et Planification**

Les modèles ont été entraînés sur une machine avec 8 GPUs NVIDIA P100. Pour les modèles de base, en utilisant les hyperparamètres décrits dans l'article, chaque étape d'entraînement a pris environ 0,4 secondes. Les modèles de base ont été entraînés pendant 100 000 étapes, soit 12 heures. Pour les grands modèles, le temps par étape était de 1,0 seconde. Les grands modèles ont été entraînés pendant 300 000 étapes (3,5 jours).

**5.3 Optimiseur**

L'optimiseur Adam a été utilisé avec $\beta_1 = 0.9$, $\beta_2 = 0.98$ et $\epsilon = 10^{-9}$. Le taux d'apprentissage a été modifié au cours de l'entraînement, selon la formule :

$lrate = d_{model}^{-0.5} \cdot \min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})$

Cela correspond à une augmentation linéaire du taux d'apprentissage pour les premières étapes d'entraînement $warmup\_steps$, puis à une diminution proportionnelle à la racine carrée inverse du numéro d'étape. La valeur $warmup\_steps = 4000$ a été utilisée.

**5.4 Régularisation**

Trois types de régularisation ont été utilisés pendant l'entraînement :

*   **Dropout Résiduel**: Le dropout a été appliqué à la sortie de chaque sous-couche, avant qu'elle ne soit ajoutée à l'entrée de la sous-couche et normalisée. De plus, le dropout a été appliqué aux sommes des embeddings et des encodages positionnels dans les piles de l'encodeur et du décodeur. Pour le modèle de base, un taux de $p_{drop} = 0.1$ a été utilisé.
*   **Lissage d'Étiquettes (Label Smoothing)**: Pendant l'entraînement, un lissage d'étiquettes avec une valeur de $\epsilon_{ls} = 0.1$ a été utilisé. Cela a nui à la perplexité, car le modèle apprend à être plus incertain, mais améliore la précision et le score BLEU.

## 6 Results

## 6 Résultats

### 6.1 Traduction Automatique

Sur la tâche de traduction de l'anglais vers l'allemand du WMT 2014, le grand modèle Transformer surpasse les meilleurs modèles précédemment rapportés, y compris les ensembles, de plus de 2.0 points BLEU, établissant un nouveau score de pointe de 28.4. L'entraînement a duré 3.5 jours sur 8 GPU P100. Même le modèle de base dépasse tous les modèles et ensembles publiés précédemment, à une fraction du coût d'entraînement.

Sur la tâche de traduction de l'anglais vers le français du WMT 2014, le grand modèle atteint un score BLEU de 41.0, surpassant tous les modèles uniques publiés précédemment, avec moins d'un quart du coût d'entraînement du précédent modèle de pointe. Le grand modèle Transformer entraîné pour l'anglais-français a utilisé un taux de dropout de 0.1 au lieu de 0.3.

Pour les modèles de base, un seul modèle obtenu en moyennant les 5 derniers points de contrôle a été utilisé. Pour les grands modèles, la moyenne des 20 derniers points de contrôle a été utilisée. La recherche par faisceau avec une taille de faisceau de 4 et une pénalité de longueur α = 0.6 a été utilisée.

Le Tableau 2 résume les résultats et compare la qualité de la traduction et les coûts d'entraînement à d'autres architectures de modèles. Le nombre d'opérations en virgule flottante utilisées pour entraîner un modèle a été estimé en multipliant le temps d'entraînement, le nombre de GPU utilisés et une estimation de la capacité en virgule flottante simple précision de chaque GPU.

### 6.2 Variations du Modèle

Pour évaluer l'importance des différents composants du Transformer, le modèle de base a été modifié de différentes manières, en mesurant le changement de performance sur la traduction anglais-allemand sur l'ensemble de développement newstest2013. La recherche par faisceau a été utilisée comme décrit précédemment, mais sans moyennage des points de contrôle.

Le Tableau 3 présente ces résultats. Les rangées (A) montrent la variation du nombre de têtes d'attention et des dimensions des clés et des valeurs, en maintenant constante la quantité de calcul. L'attention à une seule tête est 0.9 BLEU moins bonne que le meilleur réglage, et la qualité diminue également avec trop de têtes. Les rangées (B) montrent qu'une réduction de la taille des clés d'attention $d_k$ nuit à la qualité du modèle. Les rangées (C) et (D) montrent que, comme prévu, les grands modèles sont meilleurs, et le dropout est très utile pour éviter le sur-apprentissage. La rangée (E) remplace l'encodage positionnel sinusoïdal par des encodages positionnels appris, et observe des résultats presque identiques au modèle de base.

### 6.3 Analyse Syntaxique Anglaise

Pour évaluer si le Transformer peut se généraliser à d'autres tâches, des expériences sur l'analyse syntaxique anglaise ont été menées. Cette tâche présente des défis spécifiques : la sortie est soumise à de fortes contraintes structurelles et est significativement plus longue que l'entrée. De plus, les modèles RNN séquence à séquence n'ont pas atteint des résultats de pointe dans les régimes de petites données.

Un Transformer à 4 couches avec $d_{model}=1024$ a été entraîné sur la partie Wall Street Journal (WSJ) du Penn Treebank, soit environ 40 000 phrases d'entraînement. Il a également été entraîné dans un contexte semi-supervisé, en utilisant les grands corpus de haute confiance et de BerkleyParser avec environ 17 millions de phrases. Un vocabulaire de 16 000 jetons a été utilisé pour le réglage WSJ uniquement, et un vocabulaire de 32 000 jetons pour le réglage semi-supervisé.

Seul un petit nombre d'expériences ont été menées pour sélectionner le dropout, les taux d'apprentissage et la taille du faisceau sur l'ensemble de développement de la section 22, tous les autres paramètres restant inchangés par rapport au modèle de traduction de base anglais-allemand. Pendant l'inférence, la longueur maximale de la sortie a été augmentée à la longueur de l'entrée + 300. Une taille de faisceau de 21 et α = 0.3 ont été utilisés pour le réglage WSJ uniquement et le réglage semi-supervisé.

Les résultats du Tableau 4 montrent que, malgré l'absence d'ajustement spécifique à la tâche, le modèle obtient des résultats meilleurs que tous les modèles précédemment rapportés, à l'exception de la grammaire du réseau neuronal récurrent. Le Transformer surpasse le Berkeley-Parser même en entraînant uniquement sur l'ensemble d'entraînement WSJ de 40 000 phrases.

## 7 Conclusion

Dans cet article, nous avons présenté le Transformer, le premier modèle de transduction de séquences basé entièrement sur l'attention, remplaçant les couches récurrentes les plus couramment utilisées dans les architectures encodeur-décodeur par une auto-attention multi-têtes.

Pour les tâches de traduction, le Transformer peut être entraîné beaucoup plus rapidement que les architectures basées sur des couches récurrentes ou convolutionnelles. Sur les tâches de traduction WMT 2014 anglais-allemand et WMT 2014 anglais-français, nous obtenons un nouvel état de l'art. Dans la première tâche, notre meilleur modèle surpasse même tous les ensembles précédemment rapportés.

Nous sommes enthousiastes quant à l'avenir des modèles basés sur l'attention et prévoyons de les appliquer à d'autres tâches. Nous prévoyons d'étendre le Transformer à des problèmes impliquant des modalités d'entrée et de sortie autres que le texte et d'étudier les mécanismes d'attention locale et restreinte pour gérer efficacement les grandes entrées et sorties telles que les images, l'audio et la vidéo. Rendre la génération moins séquentielle est un autre de nos objectifs de recherche.

## Acknowledgements

Les auteurs remercient Nal Kalchbrenner et Stephan Gouws pour leurs commentaires, corrections et inspiration.

## References

La section "Références" contient une liste de publications académiques citées dans l'article. Ces références couvrent divers sujets liés au traitement automatique du langage naturel, à l'apprentissage profond et aux réseaux neuronaux. On y trouve des articles sur la normalisation des couches, la traduction automatique neuronale, les réseaux de mémoire, les mécanismes d'attention, les réseaux récurrents, les convolutions séparables, l'optimisation stochastique, le "dropout", le lissage d'étiquettes, l'analyse syntaxique, les modèles génératifs, les réseaux neuronaux profonds, les réseaux "LSTM", les "PCFG", et les modèles de langage. Les publications citées proviennent de conférences et de revues prestigieuses telles que NIPS, ICLR, ACL, EMNLP, et JMLR.

## Attention Visualizations

La section "Visualisations de l'Attention" présente des exemples de la manière dont le mécanisme d'attention du modèle Transformer fonctionne. La Figure 3 illustre l'attention à longue distance dans la couche d'auto-attention de l'encodeur, où plusieurs têtes d'attention se concentrent sur des dépendances distantes du verbe "making". Les Figures 4 et 5 montrent des exemples d'attention dans la couche 5 de l'encodeur, révélant comment différentes têtes d'attention semblent être impliquées dans la résolution d'anaphores et comment elles apprennent à exécuter des tâches distinctes, avec des attentions très précises pour certains mots. Ces visualisations montrent que le mécanisme d'attention permet au modèle de capturer des relations complexes entre les mots d'une phrase.
