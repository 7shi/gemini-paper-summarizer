# 제목

어텐션은 전부다 필요하다

# Abstract

지배적인 시퀀스 변환 모델은 인코더와 디코더를 포함하는 복잡한 순환 또는 컨볼루션 신경망을 기반으로 합니다. 최고 성능의 모델은 어텐션 메커니즘을 통해 인코더와 디코더를 연결하기도 합니다. 본 논문에서는 순환 및 컨볼루션을 완전히 배제하고 오직 어텐션 메커니즘만을 기반으로 하는 새로운 단순 네트워크 구조인 트랜스포머를 제안합니다. 두 가지 기계 번역 작업에 대한 실험 결과, 이러한 모델이 품질 면에서 우수하면서도 병렬화가 더 용이하고 훈련에 필요한 시간이 훨씬 적다는 것을 보여줍니다. 본 논문의 모델은 WMT 2014 영어-독일어 번역 작업에서 28.4 BLEU를 달성하여 앙상블을 포함한 기존 최고 결과를 2 BLEU 이상 개선했습니다. WMT 2014 영어-프랑스어 번역 작업에서 본 논문의 모델은 8개의 GPU에서 3.5일 동안 훈련한 후 41.8이라는 새로운 단일 모델 최고 BLEU 점수를 세웠는데, 이는 문헌에서 최고 모델의 훈련 비용의 일부에 불과합니다. 트랜스포머가 대규모 및 제한된 훈련 데이터를 모두 사용하여 영어 구성 구문 분석에 성공적으로 적용함으로써 다른 작업에도 잘 일반화된다는 것을 보여줍니다.

# 개요

본 논문에서는 순환 신경망이나 컨볼루션 신경망 대신 어텐션 메커니즘만을 사용하여 시퀀스 변환 모델을 구축하는 새로운 아키텍처인 트랜스포머를 제안하며, 기계 번역 및 구문 분석 작업에서 트랜스포머가 기존 모델보다 우수한 성능을 보이며 병렬화가 가능하고 학습 시간이 적게 소요됨을 입증합니다.

## 문제 진술

본 논문은 기존의 순환 신경망(RNN) 또는 합성곱 신경망(CNN) 기반의 시퀀스 변환 모델이 가진 계산 복잡성과 순차적인 처리 방식의 한계를 지적하며, 어텐션 메커니즘만을 사용하여 이러한 문제를 해결하는 새로운 모델인 Transformer를 제안합니다. 특히, 시퀀스 내의 장거리 의존성을 학습하는 데 어려움을 겪는 기존 모델과 달리, Transformer는 어텐션 메커니즘을 통해 입력 및 출력 시퀀스 내의 모든 위치 간의 관계를 병렬적으로 처리할 수 있도록 설계되었습니다. 또한, 기존 모델에 비해 학습 시간을 단축하고 더 나은 성능을 달성하는 것을 목표로 합니다.

## 방법론

본 논문에서는 순환 신경망(RNN)이나 합성곱 신경망(CNN) 대신 어텐션 메커니즘만을 사용하여 시퀀스 변환 모델을 구축하는 새로운 아키텍처인 트랜스포머(Transformer)를 제안합니다. 트랜스포머는 인코더와 디코더 스택으로 구성되며, 각 스택은 멀티 헤드 셀프 어텐션과 위치별 피드 포워드 네트워크로 이루어져 있습니다. 또한, 위치 정보를 추가하기 위해 positional encoding을 사용합니다. 제안된 모델은 병렬 처리가 가능하며, 기계 번역 작업에서 기존 모델보다 우수한 성능을 보입니다.

## 혁신성

이 논문에서는 순환 신경망(RNN)이나 컨볼루션 신경망(CNN)을 사용하지 않고, 오직 어텐션 메커니즘만을 사용하여 시퀀스 변환 모델을 구축하는 새로운 아키텍처인 Transformer를 제안합니다. 기존의 시퀀스 변환 모델들이 인코더와 디코더를 연결하는 데 어텐션 메커니즘을 사용했지만, Transformer는 순환 및 컨볼루션 연산을 완전히 배제하고 어텐션 메커니즘에만 의존한다는 점에서 혁신적입니다. 또한, Transformer는 병렬 처리가 훨씬 용이하여 학습 시간을 크게 단축시킬 수 있으며, 기계 번역 작업에서 기존의 최고 성능 모델들을 능가하는 성능을 달성했습니다. 특히, 영어-독일어 번역 작업에서는 앙상블 모델을 포함한 기존 최고 성능 모델보다 2 BLEU 이상 높은 점수를 기록했으며, 영어-프랑스어 번역 작업에서는 단일 모델로 새로운 최고 BLEU 점수를 달성했습니다. 또한, Transformer가 다른 작업에도 잘 일반화될 수 있음을 영어 구문 분석 작업에서 성공적으로 입증했습니다.

요약하자면, 이 논문의 혁신성은 다음과 같습니다.

1.  **순환 및 컨볼루션 연산의 배제:** 기존 시퀀스 변환 모델의 핵심 요소였던 순환 및 컨볼루션 연산을 완전히 배제하고 어텐션 메커니즘만으로 모델을 구성하여 병렬 처리를 극대화하고 학습 속도를 향상시켰습니다.
2.  **어텐션 메커니즘의 전면적 활용:** 인코더와 디코더 모두에서 어텐션 메커니즘을 핵심 요소로 사용하여 입력 및 출력 시퀀스 간의 장거리 의존성을 효과적으로 모델링했습니다.
3.  **뛰어난 성능:** 기계 번역 작업에서 기존 최고 성능 모델들을 능가하는 성능을 달성하여 새로운 최고 성능을 기록했습니다.
4.  **일반화 능력:** 다른 작업에도 잘 일반화될 수 있음을 입증하여 Transformer의 다양한 활용 가능성을 제시했습니다.

이러한 혁신적인 특징 덕분에 Transformer는 자연어 처리 분야에서 널리 사용되는 모델이 되었으며, 이후 다양한 후속 연구의 기반이 되었습니다.

# 문서 구조

- 1 Introduction
- 2 Background
- 3 Model Architecture
  - 3.1 Encoder and Decoder Stacks
  - 3.2 Attention
    - 3.2.1 Scaled Dot-Product Attention
    - 3.2.2 Multi-Head Attention
    - 3.2.3 Applications of Attention in our Model
  - 3.3 Position-wise Feed-Forward Networks
  - 3.4 Embeddings and Softmax
  - 3.5 Positional Encoding
- 4 Why Self-Attention
- 5 Training
  - 5.1 Training Data and Batching
  - 5.2 Hardware and Schedule
  - 5.3 Optimizer
  - 5.4 Regularization
- 6 Results
  - 6.1 Machine Translation
  - 6.2 Model Variations
  - 6.3 English Constituency Parsing
- 7 Conclusion
- Acknowledgements
- References
- Attention Visualizations

## 1 Introduction

본 논문에서는 순차 변환 모델의 주류를 이루는 복잡한 순환 신경망(RNN) 또는 합성곱 신경망(CNN) 구조를 벗어나, 어텐션 메커니즘만을 사용한 새로운 신경망 구조인 트랜스포머(Transformer)를 제안한다. 기존의 순환 모델은 입력 및 출력 시퀀스의 심볼 위치에 따라 계산을 순차적으로 처리하며, 이는 훈련 예제 내 병렬 처리를 어렵게 만들어 긴 시퀀스에서 메모리 제약으로 인해 배치 처리가 제한된다. 어텐션 메커니즘은 입력 또는 출력 시퀀스 내 거리와 상관없이 의존성을 모델링할 수 있게 해주지만, 대부분 순환 신경망과 함께 사용되었다. 트랜스포머는 어텐션 메커니즘만을 사용하여 입력과 출력 간의 전역 의존성을 포착하고, 병렬 처리를 극대화하여 기존 모델보다 훨씬 짧은 시간 안에 뛰어난 번역 성능을 달성할 수 있다.

## 2 Background

본 논문에서는 순차적인 계산을 줄이려는 목표가 확장된 신경 GPU, ByteNet, ConvS2S의 기본 토대임을 지적한다. 이 모델들은 컨볼루션 신경망을 기본 구성 요소로 사용하여 입력 및 출력 위치에 대한 은닉 표현을 병렬로 계산한다. ConvS2S의 경우 임의의 입력 또는 출력 위치 사이의 신호를 연결하는 데 필요한 연산 수는 위치 간 거리에 따라 선형적으로 증가하며, ByteNet의 경우 로그적으로 증가한다. 이는 먼 위치 사이의 종속성을 학습하는 것을 더 어렵게 만든다. 반면, Transformer에서는 이러한 연산 수가 상수이지만, 가중치가 적용된 위치를 평균화하여 유효 해상도가 감소하는 단점이 있다. 이러한 단점을 Multi-Head Attention을 통해 상쇄한다. 또한, 자기 주의(self-attention)는 단일 시퀀스의 여러 위치를 연결하여 시퀀스의 표현을 계산하는 데 사용되어 왔으며, 읽기 이해, 추상적 요약, 텍스트 함의, 작업 독립적인 문장 표현 학습 등 다양한 작업에서 성공적으로 사용되었다. 종단 간 메모리 네트워크는 시퀀스 정렬된 순환 대신 순환 주의 메커니즘을 기반으로 하며, 간단한 언어 질문 응답 및 언어 모델링 작업에서 좋은 성능을 보여주었다. 하지만 Transformer는 시퀀스 정렬된 RNN 또는 컨볼루션 없이 입력 및 출력 표현을 계산하기 위해 전적으로 자기 주의에 의존하는 최초의 변환 모델이다.

## 3 Model Architecture

대부분의 경쟁력 있는 신경망 시퀀스 변환 모델은 인코더-디코더 구조를 가집니다. 인코더는 심볼 표현의 입력 시퀀스 $(x_1, ..., x_n)$을 연속적인 표현 $z = (z_1, ..., z_n)$으로 매핑합니다. 디코더는 $z$가 주어지면 한 번에 하나씩 심볼의 출력 시퀀스 $(y_1, ..., y_m)$을 생성합니다. 각 단계에서 모델은 자기 회귀적이며, 다음을 생성할 때 추가 입력으로 이전에 생성된 심볼을 소비합니다.

### 3.1 인코더 및 디코더 스택

**인코더**: 인코더는 $N=6$개의 동일한 레이어 스택으로 구성됩니다. 각 레이어에는 두 개의 서브 레이어가 있습니다. 첫 번째는 멀티 헤드 셀프 어텐션 메커니즘이고, 두 번째는 간단한 위치별 완전 연결 피드 포워드 네트워크입니다. 각 서브 레이어 주위에 잔차 연결을 사용하고 그 뒤에 레이어 정규화가 이어집니다. 즉, 각 서브 레이어의 출력은 $\text{LayerNorm}(x + \text{Sublayer}(x))$입니다. 잔차 연결을 용이하게 하기 위해 모델의 모든 서브 레이어와 임베딩 레이어는 $d_{\text{model}} = 512$ 차원의 출력을 생성합니다.

**디코더**: 디코더도 $N=6$개의 동일한 레이어 스택으로 구성됩니다. 각 인코더 레이어의 두 개의 서브 레이어 외에도 디코더는 인코더 스택의 출력에 대해 멀티 헤드 어텐션을 수행하는 세 번째 서브 레이어를 삽입합니다. 인코더와 유사하게 각 서브 레이어 주위에 잔차 연결을 사용하고 그 뒤에 레이어 정규화가 이어집니다. 또한 디코더 스택의 셀프 어텐션 서브 레이어를 수정하여 위치가 후속 위치에 주의를 기울이지 않도록 합니다. 이러한 마스킹은 출력 임베딩이 한 위치만큼 오프셋된다는 사실과 결합하여 위치 $i$에 대한 예측이 $i$보다 작은 위치의 알려진 출력에만 의존할 수 있도록 합니다.

### 3.2 어텐션

어텐션 함수는 쿼리, 키-값 쌍의 집합을 출력에 매핑하는 것으로 설명할 수 있습니다. 여기서 쿼리, 키, 값 및 출력은 모두 벡터입니다. 출력은 쿼리와 해당 키의 호환성 함수에 의해 계산된 가중치가 각 값에 할당된 값의 가중 합계로 계산됩니다.

#### 3.2.1 스케일드 닷-프로덕트 어텐션

입력은 차원 $d_k$의 쿼리 및 키와 차원 $d_v$의 값으로 구성됩니다. 쿼리와 모든 키의 닷 프로덕트를 계산하고, 각각을 $\sqrt{d_k}$로 나누고, 소프트맥스 함수를 적용하여 값에 대한 가중치를 얻습니다. 실제로는 쿼리 집합에 대해 어텐션 함수를 동시에 계산합니다.

$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$

#### 3.2.2 멀티 헤드 어텐션

$d_{\text{model}}$ 차원의 키, 값 및 쿼리를 사용하여 단일 어텐션 함수를 수행하는 대신, 쿼리, 키 및 값을 각각 $d_k$, $d_k$ 및 $d_v$ 차원으로 다른 학습된 선형 투영을 사용하여 $h$번 선형 투영하는 것이 유익하다는 것을 발견했습니다. 그런 다음 이러한 투영된 쿼리, 키 및 값의 각 버전에 대해 어텐션 함수를 병렬로 수행하여 $d_v$ 차원의 출력 값을 생성합니다. 이러한 값은 연결되고 다시 투영되어 최종 값을 생성합니다.

$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$

$\text{where } \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

여기서 투영은 매개변수 행렬 $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$ 및 $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$ 입니다. 이 연구에서는 $h=8$개의 병렬 어텐션 레이어를 사용하고, 각 레이어에 대해 $d_k = d_v = d_{\text{model}}/h = 64$를 사용합니다.

#### 3.2.3 모델에서 어텐션의 응용

트랜스포머는 멀티 헤드 어텐션을 세 가지 다른 방식으로 사용합니다.

*   **"인코더-디코더 어텐션" 레이어**: 쿼리는 이전 디코더 레이어에서 가져오고, 메모리 키와 값은 인코더의 출력에서 가져옵니다. 이를 통해 디코더의 모든 위치가 입력 시퀀스의 모든 위치에 주의를 기울일 수 있습니다.

*   **인코더의 셀프 어텐션 레이어**: 키, 값 및 쿼리는 모두 동일한 위치에서 가져옵니다. 이 경우 인코더의 이전 레이어의 출력입니다. 인코더의 각 위치는 인코더의 이전 레이어의 모든 위치에 주의를 기울일 수 있습니다.

*   **디코더의 셀프 어텐션 레이어**: 디코더의 각 위치가 해당 위치를 포함하여 디코더의 모든 위치에 주의를 기울일 수 있도록 합니다. 디코더에서 왼쪽 방향 정보 흐름을 방지하기 위해 소프트맥스 입력의 모든 값을 마스킹하여 불법 연결에 해당하는 값을 $-\infty$로 설정합니다.

### 3.3 위치별 피드 포워드 네트워크

어텐션 서브 레이어 외에도 인코더 및 디코더의 각 레이어에는 각 위치에 개별적으로 동일하게 적용되는 완전 연결 피드 포워드 네트워크가 포함되어 있습니다. 이는 ReLU 활성화가 있는 두 개의 선형 변환으로 구성됩니다.

$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$

선형 변환은 위치마다 동일하지만 레이어마다 다른 매개변수를 사용합니다. 입력 및 출력의 차원은 $d_{\text{model}} = 512$이고, 내부 레이어의 차원은 $d_{ff} = 2048$입니다.

### 3.4 임베딩 및 소프트맥스

다른 시퀀스 변환 모델과 유사하게 학습된 임베딩을 사용하여 입력 토큰과 출력 토큰을 $d_{\text{model}}$ 차원의 벡터로 변환합니다. 또한 일반적인 학습된 선형 변환과 소프트맥스 함수를 사용하여 디코더 출력을 예측된 다음 토큰 확률로 변환합니다. 모델에서 두 임베딩 레이어와 프리 소프트맥스 선형 변환 사이에서 동일한 가중치 행렬을 공유합니다. 임베딩 레이어에서 해당 가중치에 $\sqrt{d_{\text{model}}}$을 곱합니다.

### 3.5 위치 인코딩

모델에는 순환이나 컨볼루션이 포함되어 있지 않으므로 모델이 시퀀스 순서를 사용하려면 시퀀스에서 토큰의 상대적 또는 절대적 위치에 대한 일부 정보를 주입해야 합니다. 이를 위해 인코더 및 디코더 스택 하단에 입력 임베딩에 "위치 인코딩"을 추가합니다. 위치 인코딩은 임베딩과 동일한 차원 $d_{\text{model}}$을 가지므로 두 가지를 합산할 수 있습니다. 이 연구에서는 서로 다른 주파수의 사인 및 코사인 함수를 사용합니다.

$PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{\text{model}}})$

$PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{\text{model}}})$

여기서 $pos$는 위치이고 $i$는 차원입니다. 즉, 위치 인코딩의 각 차원은 사인파에 해당합니다. 파장은 $2\pi$에서 $10000 \cdot 2\pi$까지 기하급수적으로 형성됩니다.

## 4 Why Self-Attention

본 논문에서는 self-attention 레이어가 순환(recurrent) 및 컨볼루션 레이어와 비교하여 가지는 이점을 세 가지 측면에서 분석한다. 첫째, 레이어당 계산 복잡도(computational complexity)를 비교한다. 둘째, 병렬화 가능성(parallelizability)을 측정하기 위해 필요한 최소 순차 연산(sequential operation) 수를 비교한다. 셋째, 네트워크 내 장거리 의존성(long-range dependencies) 학습에 영향을 미치는 경로 길이(path length)를 비교한다. Self-attention 레이어는 모든 위치를 상수 시간 내에 연결하는 반면, 순환 레이어는 $O(n)$의 순차 연산을 요구한다. 계산 복잡도 측면에서 self-attention 레이어는 시퀀스 길이 $n$이 표현 차원 $d$보다 작을 때 더 빠르다. 컨볼루션 레이어는 커널 크기 $k$에 따라 다르지만, self-attention 레이어와 유사한 복잡도를 가진다. 장거리 의존성 학습 측면에서 self-attention 레이어는 입력과 출력 위치 사이의 최대 경로 길이가 상수인 반면, 순환 레이어는 $O(n)$, 컨볼루션 레이어는 $O(n/k)$ 또는 $O(log_k(n))$이다.  Self-attention 레이어는 해석 가능성(interpretability) 측면에서도 장점을 가지며, attention 분포를 통해 모델이 문장 구조 및 의미를 학습하는 방식을 파악할 수 있다.

## 5 Training

본 섹션에서는 모델 훈련에 사용된 훈련 방식에 대해 설명한다.

### 5.1 훈련 데이터 및 배치

*   **데이터셋:** 영어-독일어 번역에는 WMT 2014 데이터셋(약 450만 문장 쌍)을 사용했고, 영어-프랑스어 번역에는 더 큰 WMT 2014 데이터셋(약 3600만 문장)을 사용했다.
*   **인코딩:** 문장은 바이트 쌍 인코딩(BPE)을 사용하여 인코딩했으며, 영어-독일어는 약 37,000개의 토큰으로 구성된 공유 어휘를, 영어-프랑스어는 32,000개의 단어 조각 어휘를 사용했다.
*   **배치:** 문장 쌍은 대략적인 시퀀스 길이에 따라 배치로 묶었다. 각 훈련 배치는 약 25,000개의 소스 토큰과 25,000개의 타겟 토큰을 포함했다.

### 5.2 하드웨어 및 스케줄

*   **하드웨어:** 모델은 8개의 NVIDIA P100 GPU가 장착된 컴퓨터에서 훈련되었다.
*   **훈련 시간:** 기본 모델은 각 훈련 단계가 약 0.4초 걸렸고, 총 100,000단계(12시간) 동안 훈련되었다. 큰 모델은 각 단계가 1.0초 걸렸고, 총 300,000단계(3.5일) 동안 훈련되었다.

### 5.3 옵티마이저

*   **옵티마이저:** Adam 옵티마이저를 사용했으며, $\beta_1 = 0.9$, $\beta_2 = 0.98$, $\epsilon = 10^{-9}$로 설정했다.
*   **학습률 스케줄:** 학습률은 다음과 같은 공식에 따라 훈련 과정에서 조정되었다.

    $lrate = d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})$
    
    여기서 $warmup\_steps = 4000$이다. 학습률은 처음 $warmup\_steps$ 단계 동안 선형적으로 증가하고, 그 후 단계 수의 역제곱근에 비례하여 감소한다.

### 5.4 정규화

훈련 중에는 세 가지 유형의 정규화 기법을 사용했다.

*   **잔차 드롭아웃:** 각 서브 레이어의 출력에 드롭아웃을 적용하고, 서브 레이어 입력과 합산하고 정규화하기 전에 적용했다. 또한 인코더와 디코더 스택 모두에서 임베딩과 위치 인코딩의 합에도 드롭아웃을 적용했다. 기본 모델의 경우 드롭아웃 비율은 $p_{drop} = 0.1$이었다.
*   **레이블 스무딩:** 레이블 스무딩 값 $\epsilon_{ls} = 0.1$을 사용했다. 이는 모델이 더 불확실하게 학습하도록 유도하여 퍼플렉시티는 저하되지만 정확도와 BLEU 점수는 향상시킨다.

## 6 Results

### 6.1 기계 번역

WMT 2014 영어-독일어 번역 과제에서, 큰 트랜스포머 모델은 기존의 최고 성능 모델(앙상블 포함)보다 2.0 BLEU 이상 높은 28.4의 새로운 최고 BLEU 점수를 달성했습니다. 이 모델은 8개의 P100 GPU에서 3.5일 동안 학습되었습니다. 기본 모델도 기존의 모든 모델과 앙상블을 능가하며, 경쟁 모델보다 훨씬 적은 학습 비용이 들었습니다. WMT 2014 영어-프랑스어 번역 과제에서, 큰 모델은 41.0의 BLEU 점수를 달성하여 기존의 단일 모델들을 능가했으며, 이전 최고 성능 모델의 1/4 미만의 학습 비용으로 달성했습니다. 영어-프랑스어 모델은 드롭아웃 비율을 0.1로 사용했습니다. 기본 모델은 마지막 5개 체크포인트를 평균화하여 얻었고, 큰 모델은 마지막 20개 체크포인트를 평균화했습니다. 빔 크기는 4, 길이 페널티는 0.6을 사용했습니다. 추론 시 최대 출력 길이는 입력 길이 + 50으로 설정했지만, 가능하면 일찍 종료했습니다. 표 2는 다른 모델 아키텍처와 번역 품질 및 학습 비용을 비교한 결과를 요약합니다.

### 6.2 모델 변형

트랜스포머의 다양한 구성 요소의 중요성을 평가하기 위해, 기본 모델을 여러 방식으로 변경하여 영어-독일어 번역 성능 변화를 측정했습니다(newstest2013 개발 세트 사용). 표 3은 이러한 결과를 보여줍니다. 표 3의 (A) 행에서는 계산량을 일정하게 유지하면서 어텐션 헤드 수와 키/값 차원을 변경했습니다. 단일 헤드 어텐션은 최고 설정보다 0.9 BLEU 낮았고, 헤드 수가 너무 많아도 품질이 떨어졌습니다. (B) 행에서는 어텐션 키 크기($d_k$)를 줄이면 모델 품질이 저하되는 것을 확인했습니다. (C)와 (D) 행에서는 예상대로 더 큰 모델이 더 나은 성능을 보이고, 드롭아웃이 과적합을 피하는 데 매우 유용하다는 것을 확인했습니다. (E) 행에서는 사인파 위치 인코딩을 학습된 위치 임베딩으로 대체했을 때 기본 모델과 거의 동일한 결과가 나왔습니다.

### 6.3 영어 구문 분석

트랜스포머가 다른 작업에도 일반화될 수 있는지 평가하기 위해 영어 구문 분석 실험을 수행했습니다. 이 작업은 강한 구조적 제약 조건이 있으며 입력보다 훨씬 긴 출력을 생성해야 합니다. 또한 RNN 기반 모델은 소량의 데이터에서 최고 성능을 달성하지 못했습니다. Penn Treebank의 Wall Street Journal(WSJ) 부분(약 40,000개 문장)에서 $d_{model}$ = 1024인 4계층 트랜스포머를 학습했습니다. 또한 약 1,700만 개의 문장이 있는 더 큰 고신뢰도 BerkeleyParser 코퍼스를 사용하여 준지도 학습 설정에서 학습했습니다. WSJ 전용 설정에는 16,000개 토큰의 어휘를 사용하고, 준지도 설정에는 32,000개 토큰의 어휘를 사용했습니다. 섹션 22 개발 세트에서 드롭아웃, 학습률, 빔 크기를 선택했습니다. 표 4는 WSJ 섹션 23에서 결과를 보여줍니다. 작업별 튜닝이 부족했음에도 불구하고, 트랜스포머는 Recurrent Neural Network Grammar를 제외한 기존의 모든 모델보다 나은 성능을 보였습니다. RNN 모델과 달리, 트랜스포머는 40,000개 문장만으로 학습했을 때도 Berkeley-Parser를 능가했습니다.

## 7 Conclusion

본 논문에서는 Transformer 모델을 제시하였으며, 이는 어텐션 메커니즘만을 사용하여 인코더-디코더 구조에서 순환 레이어를 대체하는 최초의 시퀀스 변환 모델입니다. Transformer는 순환 또는 컨볼루션 레이어 기반 모델보다 훨씬 빠르게 학습될 수 있으며, WMT 2014 영어-독일어 및 영어-프랑스어 번역 작업에서 새로운 최고 성능을 달성했습니다. 특히 영어-독일어 번역에서는 앙상블 모델을 포함한 이전의 모든 모델을 능가하는 결과를 보였습니다. 저자들은 어텐션 기반 모델의 미래에 대한 기대감을 표하며, Transformer를 텍스트 외 다른 입력 및 출력 양식(이미지, 오디오, 비디오 등)을 포함한 다양한 작업으로 확장하고, 지역적 또는 제한된 어텐션 메커니즘을 연구하여 대규모 입력 및 출력을 효율적으로 처리하는 것을 목표로 하고 있습니다. 또한, 생성 과정을 덜 순차적으로 만드는 것도 연구 목표 중 하나입니다.

## Acknowledgements

저자들은 Nal Kalchbrenner와 Stephan Gouws의 유익한 의견, 수정 및 영감에 감사를 표한다.

## References

본 논문은 기계 번역 및 자연어 처리 분야에서 중요한 참고 문헌들을 인용하고 있습니다. 주요 내용은 다음과 같습니다.

* **레이어 정규화 (Layer Normalization)**: [1]에서는 신경망 학습을 안정화시키는 레이어 정규화 기법을 소개합니다.
* **어텐션 메커니즘 (Attention Mechanism)**: [2], [19], [24]에서는 기계 번역 모델에서 어텐션 메커니즘의 중요성을 강조하며, 입력 및 출력 시퀀스 간의 관계를 모델링하는 데 사용됨을 보여줍니다.
* **RNN (Recurrent Neural Network) 및 LSTM (Long Short-Term Memory)**: [4], [5], [7], [13], [21]에서는 순환 신경망과 LSTM이 시퀀스 모델링에 효과적임을 설명하며, 이러한 모델들이 언어 모델링 및 기계 번역과 같은 작업에서 어떻게 사용되는지 보여줍니다.
* **CNN (Convolutional Neural Network)**: [6], [9]에서는 CNN을 사용하여 시퀀스 모델링을 수행하는 방법을 제시하며, 특히 병렬 처리가 가능한 구조를 강조합니다.
* **트랜스포머 (Transformer) 모델의 선행 연구**: [16], [17], [18]에서는 트랜스포머 모델의 기반이 되는 연구들을 소개하며, 어텐션 메커니즘을 활용한 다양한 시퀀스 모델링 접근법을 제시합니다.
* **기계 번역 (Machine Translation)**: [3], [5], [23], [31], [38], [39]에서는 다양한 기계 번역 모델 및 접근법을 제시하며, 특히 구글의 신경망 기계 번역 시스템을 포함한 최신 기술 동향을 보여줍니다.
* **구문 분석 (Parsing)**: [8], [14], [25], [26], [29], [37], [40]에서는 구문 분석 모델 및 기법을 소개하며, 특히 신경망을 이용한 구문 분석 접근법을 강조합니다.
* **기타 자연어 처리 작업**: [22], [27], [28], [30], [34]에서는 텍스트 요약, 문장 임베딩, 질의 응답 등 다양한 자연어 처리 작업에 대한 연구를 소개합니다.
* **정규화 및 최적화**: [20], [33], [36]에서는 드롭아웃, 레이블 스무딩 등 신경망 학습에서 사용되는 정규화 기법과 Adam과 같은 최적화 알고리즘을 소개합니다.

이러한 참고 문헌들은 본 논문에서 제시하는 트랜스포머 모델의 이론적 배경과 실험적 근거를 제공하며, 자연어 처리 분야의 중요한 연구 동향을 보여줍니다.

## Attention Visualizations

본 논문에서는 Transformer 모델의 self-attention 메커니즘을 시각화하여 분석하였다. 특히, 인코더의 self-attention 레이어에서 장거리 의존성을 포착하는 능력을 강조하였다.

**그림 3**은 6개 레이어 중 5번째 레이어의 self-attention을 보여준다. "making"이라는 단어에 대한 attention을 시각화한 것으로, 여러 attention 헤드가 "making...more difficult" 구문을 완성하기 위해 멀리 떨어진 단어에 집중하는 것을 보여준다. 각 헤드는 다른 색으로 표현되어 있다.

**그림 4**는 6개 레이어 중 5번째 레이어에서 anaphora 해결에 관여하는 두 개의 attention 헤드를 보여준다. 상단은 전체 attention을 보여주고, 하단은 "its"라는 단어에 대한 attention을 분리하여 보여준다. 이 그림에서 attention이 특정 단어에 매우 집중되어 있는 것을 확인할 수 있다.

**그림 5**는 문장 구조와 관련된 attention 헤드의 동작을 보여준다. 인코더의 self-attention 레이어에서 두 개의 다른 헤드를 시각화하여, 각 헤드가 서로 다른 역할을 수행하는 것을 보여준다.

이러한 시각화는 Transformer 모델이 문장 내의 장거리 의존성을 효과적으로 학습하고, 각 attention 헤드가 서로 다른 작업을 수행한다는 것을 보여준다.
