# Abstract

支配的な系列変換モデルは、エンコーダとデコーダを含む複雑な再帰型または畳み込みニューラルネットワークに基づいている。 最も性能の高いモデルは、注意機構を通じてエンコーダとデコーダを接続する。 本稿では、再帰と畳み込みを完全に排除し、注意機構のみに基づく新しいシンプルなネットワークアーキテクチャであるTransformerを提案する。 2つの機械翻訳タスクに関する実験では、これらのモデルがより並列化可能であり、トレーニングに必要な時間が大幅に少ないにもかかわらず、品質が優れていることが示されている。 本モデルは、WMT 2014の英語からドイツ語への翻訳タスクで28.4 BLEUを達成し、アンサンブルを含む既存の最高の結果を2 BLEU以上上回っている。 WMT 2014の英語からフランス語への翻訳タスクでは、本モデルは8つのGPUで3.5日間トレーニングした後、41.8という新しい単一モデルの最先端のBLEUスコアを確立し、文献にある最高のモデルのトレーニングコストのほんの一部である。 Transformerが、大規模および限定的なトレーニングデータを用いた英語の構成要素解析に適用することで、他のタスクにもうまく一般化できることを示す。

# 概要

本論文では、再帰型ニューラルネットワークや畳み込みニューラルネットワークに代わり、アテンション機構のみを用いた新しいネットワーク構造であるTransformerを提案し、機械翻訳タスクにおいて優れた性能と並列化による高速な学習を達成したことを示す。

## 問題意識

この論文は、従来のシーケンス変換モデルが抱える、複雑な再帰型または畳み込みニューラルネットワークに基づくという問題を解決しようとしている。これらのモデルは、エンコーダとデコーダを含み、さらにアテンション機構を介して接続されているが、計算の並列化が難しく、学習に時間がかかるという課題があった。そこで、この論文では、再帰や畳み込みを一切使用せず、アテンション機構のみに基づいた新しいネットワークアーキテクチャであるTransformerを提案し、機械翻訳タスクにおいて、より高品質で並列化可能、かつ学習時間を大幅に短縮できるモデルの実現を目指している。

## 手法

この論文では、Transformerという新しいネットワークアーキテクチャを提案している。Transformerは、従来のリカレントニューラルネットワークや畳み込みニューラルネットワークに代わり、アテンションメカニズムのみに基づいて構築されている。具体的には、エンコーダとデコーダの両方に、自己アテンションと点ごとの全結合層を積み重ねた構造を採用している。これにより、並列化が大幅に進み、学習時間を短縮できる。また、翻訳タスクにおいて、既存のモデルを上回る性能を達成している。

## 新規性

本論文の新規性は、以下の3点に集約される。

1.  **Transformerアーキテクチャの導入**: 
    *   従来の系列変換モデルで主流であったRNNやCNNを完全に排除し、アテンション機構のみに基づく新しいネットワークアーキテクチャであるTransformerを提案した点。これにより、並列計算が可能となり、学習時間を大幅に短縮した。

2.  **自己アテンションの活用**:
    *   入力と出力の表現を計算する際に、系列に沿ったRNNや畳み込み演算ではなく、自己アテンションのみを用いた初のモデルである点。これにより、長距離の依存関係を捉えることが可能になった。

3.  **機械翻訳における最先端性能の達成**:
    *   WMT 2014の英語-ドイツ語翻訳タスクにおいて、既存の最良モデル（アンサンブルを含む）を2 BLEU以上上回る28.4 BLEUを達成した点。
    *   WMT 2014の英語-フランス語翻訳タスクにおいて、既存の単一モデルの最先端性能を大幅に上回る41.8 BLEUを達成した点。
    *   これらの成果を、既存の最良モデルよりも大幅に少ない計算コストで達成した点。

これらの新規性により、Transformerは機械翻訳だけでなく、他のタスク（構文解析など）にも応用可能な汎用的なモデルであることを示唆している。

# 章構成

- 1 Introduction
- 2 Background
- 3 Model Architecture
  - 3.1 Encoder and Decoder Stacks
  - 3.2 Attention
    - 3.2.1 Scaled Dot-Product Attention
    - 3.2.2 Multi-Head Attention
    - 3.2.3 Applications of Attention in our Model
  - 3.3 Position-wise Feed-Forward Networks
  - 3.4 Embeddings and Softmax
  - 3.5 Positional Encoding
- 4 Why Self-Attention
- 5 Training
  - 5.1 Training Data and Batching
  - 5.2 Hardware and Schedule
  - 5.3 Optimizer
  - 5.4 Regularization
- 6 Results
  - 6.1 Machine Translation
  - 6.2 Model Variations
  - 6.3 English Constituency Parsing
- 7 Conclusion
- Acknowledgements
- References
- Attention Visualizations

## 1 Introduction

系列モデリングと変換問題において、リカレントニューラルネットワーク（RNN）、特に長短期記憶（LSTM）やゲート付きリカレントユニット（GRU）が最先端の手法として確立されている。これらのモデルは、入力と出力のシンボル位置に沿って計算を分解し、前の隠れ状態と現在の入力に基づいて隠れ状態の系列を生成する。この本質的な逐次処理のため、学習例内での並列化が妨げられ、長い系列長ではメモリ制約がバッチ処理を制限する。最近の研究では、因数分解や条件付き計算によって計算効率が向上しているが、逐次計算の根本的な制約は残っている。

アテンション機構は、入力や出力系列における距離に関わらず依存関係をモデル化することを可能にし、様々なタスクで重要な役割を果たしている。しかし、ほとんどの場合、これらのアテンション機構はリカレントネットワークと組み合わせて使用されている。

本研究では、リカレンスを避け、アテンション機構のみに依存して入力と出力間のグローバルな依存関係を捉える新しいモデルアーキテクチャであるTransformerを提案する。Transformerは、並列化を大幅に向上させ、わずか12時間のトレーニングで最先端の翻訳品質を達成できる。

## 2 Background

系列計算の削減は、Extended Neural GPU、ByteNet、ConvS2S の基礎でもある。これらは畳み込みニューラルネットワークを基本構成要素とし、全ての入出力位置に対して並列に隠れ表現を計算する。これらのモデルでは、任意の位置間の信号を関連付けるのに必要な演算数は、ConvS2Sでは線形に、ByteNetでは対数的に増加する。これにより、遠い位置間の依存関係の学習が困難になる。Transformerでは、この演算数は一定になるが、注意機構で重み付けされた位置を平均化するため、有効な解像度が低下する。この効果は、セクション 3.2 で説明する Multi-Head Attention で打ち消される。自己注意（イントラ注意とも呼ばれる）は、系列の表現を計算するために、単一の系列の異なる位置を関連付ける注意機構である。自己注意は、読解、抽象的な要約、テキストの含意、タスクに依存しない文表現の学習など、様々なタスクで成功を収めている。エンドツーエンドのメモリネットワークは、系列に沿った再帰の代わりに再帰的な注意機構に基づいており、簡単な言語の質問応答や言語モデリングタスクで良好な性能を示すことが示されている。Transformerは、系列に沿ったRNNや畳み込みを使用せずに、入力と出力の表現を計算するために自己注意のみに依存した最初の変換モデルである。

## 3 Model Architecture

### 3. モデルアーキテクチャ

競争力の高いニューラル系列変換モデルの多くは、エンコーダ・デコーダ構造を持つ。エンコーダはシンボル表現の入力系列 $(x _ 1, ..., x _ n)$ を連続表現の系列 $z = (z _ 1, ..., z _ n)$ にマッピングする。デコーダは $z$ が与えられると、シンボルの出力系列 $(y _ 1, ..., y _ m)$ を一度に1要素ずつ生成する。各ステップで、モデルは自己回帰的であり、次の生成時に以前に生成されたシンボルを追加の入力として消費する。

#### 3.1 エンコーダとデコーダのスタック

**エンコーダ**: エンコーダは $N=6$ 個の同一の層のスタックで構成される。各層には2つのサブ層がある。1つ目はマルチヘッド自己注意機構で、2つ目は単純な位置ごとの全結合フィードフォワードネットワークである。各サブ層の周りに残差接続を使用し、その後に層正規化を行う。つまり、各サブ層の出力は $LayerNorm(x + Sublayer(x))$ となる。ここで $Sublayer(x)$ はサブ層自体によって実装される関数である。これらの残差接続を容易にするために、モデル内のすべてのサブ層と埋め込み層は、次元 $d _ {model} = 512$ の出力を生成する。

**デコーダ**: デコーダも $N=6$ 個の同一の層のスタックで構成される。各エンコーダ層の2つのサブ層に加えて、デコーダは3番目のサブ層を挿入し、これはエンコーダスタックの出力に対してマルチヘッド注意を実行する。エンコーダと同様に、各サブ層の周りに残差接続を使用し、その後に層正規化を行う。また、デコーダスタックの自己注意サブ層を修正して、位置が後続の位置に注意を払うのを防ぐ。このマスキングは、出力埋め込みが1つの位置だけオフセットされているという事実と組み合わされて、位置 $i$ の予測が位置 $i$ 未満の既知の出力にのみ依存するようにする。

#### 3.2 注意機構

注意関数は、クエリとキーと値のペアのセットを出力にマッピングするものとして記述できる。ここで、クエリ、キー、値、出力はすべてベクトルである。出力は、対応するキーを持つクエリの適合性関数によって計算された、各値に割り当てられた重みによる値の加重和として計算される。

##### 3.2.1 スケール化ドット積注意

特定のアテンションを「スケール化ドット積注意」と呼ぶ。入力は次元 $d _ k$ のクエリとキー、および次元 $d _ v$ の値で構成される。クエリとすべてのキーのドット積を計算し、それぞれを $\sqrt{d _ k}$ で割り、softmax関数を適用して値の重みを取得する。実際には、クエリのセットに対して注意関数を同時に計算し、行列 $Q$ にまとめられる。キーと値も行列 $K$ と $V$ にまとめられる。出力行列は次のように計算される。

$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

最も一般的に使用される2つの注意関数は、加法注意とドット積（乗法）注意である。加法注意は、単一の隠れ層を持つフィードフォワードネットワークを使用して適合性関数を計算する。2つは理論的な複雑さが似ているが、ドット積注意は、高度に最適化された行列乗算コードを使用して実装できるため、実際にははるかに高速でスペース効率が高い。

$d _ k$ の値が小さい場合は、2つのメカニズムは同様に機能するが、 $d _ k$ の値が大きい場合は、加法注意はスケーリングなしでドット積注意よりも優れている。これは $d _ k$ の値が大きい場合、ドット積が大きくなり、softmax関数を勾配が非常に小さい領域に押し込むためと考えられる。この影響に対抗するために、ドット積を $\sqrt{d _ k}$ でスケーリングする。

##### 3.2.2 マルチヘッド注意

$d _ {model}$ 次元のキー、値、クエリを持つ単一の注意関数を実行する代わりに、クエリ、キー、値をそれぞれ異なる学習された線形射影で $h$ 回線形射影することが有益であることがわかった。これらの射影されたバージョンのクエリ、キー、値のそれぞれに対して、注意関数を並行して実行し、 $d _ v$ 次元の出力が得られる。

これらの出力値は連結され、再度射影されて、図2に示すように最終的な値が得られる。マルチヘッド注意を使用すると、モデルは異なる位置の異なる表現サブスペースからの情報に共同で注意を払うことができる。単一の注意ヘッドを使用すると、平均化によりこれが抑制される。

$MultiHead(Q, K, V) = Concat(head _ 1, ..., head _ h)W^O$

ここで $head _ i = Attention(QW _ i^Q, KW _ i^K, VW _ i^V)$

射影はパラメータ行列 $W _ i^Q \in R^{d _ {model} \times d _ k}$, $W _ i^K \in R^{d _ {model} \times d_k}$, $W _ i^V \in R^{d _ {model} \times d _ v}$ および $W^O \in R^{hd _ v \times d _ {model}}$ である。

この論文では $h=8$ 個の並列注意層、つまりヘッドを使用する。これらのそれぞれに対して $d_k = d_v = d_{model}/h = 64$ を使用する。各ヘッドの次元が縮小されているため、総計算コストは、完全な次元を持つ単一ヘッド注意の計算コストとほぼ同じである。

##### 3.2.3 モデルにおける注意の応用

Transformerは、マルチヘッド注意を3つの異なる方法で使用する。

*   「エンコーダ・デコーダ注意」層では、クエリは前のデコーダ層から来て、メモリキーと値はエンコーダの出力から来る。これにより、デコーダ内のすべての位置が入力シーケンス内のすべての位置に注意を払うことができる。これは、[38, 2, 9]などのシーケンス・ツー・シーケンスモデルの典型的なエンコーダ・デコーダ注意機構を模倣している。
*   エンコーダには自己注意層が含まれている。自己注意層では、すべてのキー、値、クエリは同じ場所、この場合はエンコーダの前の層の出力から来る。エンコーダ内の各位置は、エンコーダの前の層内のすべての位置に注意を払うことができる。
*   同様に、デコーダの自己注意層では、デコーダ内の各位置が、その位置までのデコーダ内のすべての位置に注意を払うことができる。自己回帰プロパティを維持するために、デコーダでの左方向の情報フローを防ぐ必要がある。これは、図2に示すように、違法な接続に対応するsoftmaxの入力のすべての値をマスキングする（−∞に設定する）ことによってスケール化ドット積注意の内部に実装する。

#### 3.3 位置ごとのフィードフォワードネットワーク

注意サブ層に加えて、エンコーダとデコーダの各層には、各位置に個別かつ同一に適用される全結合フィードフォワードネットワークが含まれている。これは、ReLU活性化を挟んだ2つの線形変換で構成される。

$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$

線形変換は異なる位置で同じであるが、層ごとに異なるパラメータを使用する。これを記述する別の方法は、カーネルサイズ1の2つの畳み込みとして記述することである。入力と出力の次元は $d _ {model} = 512$ であり、中間層の次元は $d _ {ff} = 2048$ である。

#### 3.4 埋め込みとソフトマックス

他のシーケンス変換モデルと同様に、学習された埋め込みを使用して、入力トークンと出力トークンを次元 $d _ {model}$ のベクトルに変換する。また、通常の学習された線形変換とsoftmax関数を使用して、デコーダ出力を予測された次のトークンの確率に変換する。このモデルでは、[30]と同様に、2つの埋め込み層とプレソフトマックス線形変換の間で同じ重み行列を共有する。埋め込み層では、これらの重みに $\sqrt{d _ {model}}$ を掛ける。

#### 3.5 位置エンコーディング

モデルには回帰も畳み込みも含まれていないため、モデルがシーケンスの順序を利用できるようにするには、シーケンス内のトークンの相対位置または絶対位置に関する情報を注入する必要がある。このために、エンコーダとデコーダスタックの下部にある入力埋め込みに「位置エンコーディング」を追加する。位置エンコーディングは、埋め込みと同じ次元 $d _ {model}$ を持つため、2つを合計できる。位置エンコーディングには、学習されたものと固定されたものなど、多くの選択肢がある[9]。

この論文では、異なる周波数の正弦関数と余弦関数を使用する。

$PE _ {(pos, 2i)} = sin(pos/10000^{2i/d _ {model}})$

$PE _ {(pos, 2i+1)} = cos(pos/10000^{2i/d _ {model}})$

ここで、 $pos$ は位置、 $i$ は次元である。つまり、位置エンコーディングの各次元は正弦波に対応する。波長は2πから10000・2πまでの幾何級数を形成する。この関数を選択したのは、固定オフセット $k$ に対して、 $PE _ {pos+k}$ が $PE _ {pos}$ の線形関数として表現できるため、モデルが相対位置によって注意を払うことを容易に学習できると仮定したからである。

代わりに学習された位置埋め込み[9]を使用する実験も行い、2つのバージョンがほぼ同一の結果を生成することを発見した（表3の行（E）を参照）。正弦波バージョンを選択したのは、トレーニング中に発生した長さよりも長いシーケンス長にモデルを外挿できる可能性があるためである。

## 4 Why Self-Attention

本論文では、自己注意機構の様々な側面を、系列変換モデルで一般的に用いられる再帰型層や畳み込み層と比較検討する。自己注意機構の利用を動機づけるために、以下の3つの望ましい性質を考慮する。

1.  **層ごとの計算複雑性**: 自己注意層は、入力系列の長さ $n$ に対して $O(n^2d)$ の計算量を必要とする。ここで、 $d$ は表現の次元である。再帰型層は $O(nd^2)$、畳み込み層は $O(knd^2)$ (ここで $k$ はカーネルサイズ) の計算量を必要とする。自己注意層は、系列長 $n$ が表現次元 $d$ よりも小さい場合に、再帰型層よりも高速になる。

2.  **並列化の可能性**: 自己注意層は、系列内のすべての位置に対して並列に計算できるため、必要な逐次演算の回数は $O(1)$ である。再帰型層は $O(n)$ の逐次演算を必要とし、畳み込み層は $O(1)$ の逐次演算を必要とする（ただし、畳み込み層はカーネルサイズに依存する）。

3.  **長距離依存性のパス長**: 長距離依存性を学習する能力は、ネットワーク内の順方向および逆方向の信号が通過する必要があるパスの長さに影響される。自己注意層は、すべての位置を一定の逐次演算回数で接続するため $O(1)$ のパス長を持つ。再帰型層は $O(n)$ のパス長を持ち、畳み込み層は $O(log _ k(n))$ のパス長を持つ。

自己注意層は、系列内のすべての位置を一定の逐次演算回数で接続できるため、長距離依存性を捉えるのに有利である。また、自己注意層は、再帰型層よりも並列化に適しており、計算効率が高い。

## 5 Training

**5. 訓練**

このセクションでは、モデルの訓練体制について述べる。

**5.1 訓練データとバッチ処理**

*   WMT 2014 英語-ドイツ語データセット（約450万文ペア）で訓練を実施。
*   文は、約37,000トークンの共有ソース・ターゲット語彙を持つバイトペアエンコーディングを用いてエンコード。
*   英語-フランス語には、3600万文からなる大規模なWMT 2014データセットを使用し、32,000語のワードピース語彙に分割。
*   文ペアは、近似的な文長によってバッチ処理。各訓練バッチには、約25,000のソーストークンと25,000のターゲットトークンを含む文ペアのセットが含まれる。

**5.2 ハードウェアとスケジュール**

*   モデルは、8基のNVIDIA P100 GPUを搭載したマシンで訓練。
*   論文全体で記述されているハイパーパラメータを使用したベースモデルの場合、各訓練ステップは約0.4秒。ベースモデルは合計100,000ステップ（12時間）訓練。
*   ビッグモデルの場合、ステップ時間は1.0秒。ビッグモデルは300,000ステップ（3.5日）訓練。

**5.3 オプティマイザー**

*   Adamオプティマイザーを使用 ($\beta_1 = 0.9$, $\beta_2 = 0.98$, $\epsilon = 10^{-9}$) 。
*   学習率は、以下の式に従って訓練中に変化させる。  
    $\text{lrate} = d _ {\text{model}}^{-0.5} \cdot \min(\text{step\\_num}^{-0.5}, \text{step\\_num} \cdot \text{warmup\\_steps}^{-1.5})$
*   学習率は、最初の`warmup_steps`訓練ステップでは線形に増加し、その後はステップ数の逆平方根に比例して減少。
*   `warmup_steps`は4000に設定。

**5.4 正則化**

訓練中に3種類の正則化を採用。

*   **残差ドロップアウト:** 各サブレイヤーの出力にドロップアウトを適用し、サブレイヤー入力に追加して正規化。また、エンコーダとデコーダスタックの両方で、埋め込みと位置エンコーディングの合計にもドロップアウトを適用。ベースモデルの場合、ドロップアウト率は $p_{drop} = 0.1$ 。
*   **ラベルスムージング:** 訓練中に $\epsilon _ {ls} = 0.1$ のラベルスムージングを採用。これにより、モデルはより不確実になることを学習するが、精度とBLEUスコアが向上。

## 6 Results

## 6 結果

### 6.1 機械翻訳

WMT2014 英語-ドイツ語翻訳タスクにおいて、本論文で提案された Transformer (big) モデルは、既存の最高性能モデル（アンサンブルを含む）を 2.0 BLEU 以上上回り、28.4 という新たな最高 BLEU スコアを達成した。このモデルは、8 つの P100 GPU で 3.5 日間学習された。ベースモデルでも、既存の競合モデルを大幅に上回る性能を、より少ない学習コストで達成している。

WMT2014 英語-フランス語翻訳タスクでは、Transformer (big) モデルは 41.0 の BLEU スコアを達成し、既存の単一モデルを上回る性能を、既存の最高性能モデルの 1/4 未満の学習コストで達成した。英語-フランス語翻訳用に学習された Transformer (big) モデルでは、ドロップアウト率 $P_{drop}$ は 0.3 ではなく 0.1 が使用された。

ベースモデルでは、10 分間隔で書き込まれた最後の 5 つのチェックポイントを平均化した単一モデルを使用した。大規模モデルでは、最後の 20 個のチェックポイントを平均化した。ビームサイズ 4、長さペナルティ $\alpha$ = 0.6 のビームサーチを使用した。これらのハイパーパラメータは、開発セットでの実験後に選択された。推論中の最大出力長は入力長 + 50 に設定されたが、可能な限り早期に終了した。

表 2 は、結果を要約し、翻訳品質と学習コストを他のモデルアーキテクチャと比較している。モデルの学習に使用される浮動小数点演算の数は、学習時間、使用する GPU の数、および各 GPU の持続的な単精度浮動小数点容量の見積もりを乗算することにより推定した。

### 6.2 モデルのバリエーション

Transformer のさまざまなコンポーネントの重要性を評価するために、ベースモデルをさまざまな方法で変更し、英語-ドイツ語翻訳の性能の変化を newstest2013 開発セットで測定した。ビームサーチを使用したが、チェックポイントの平均化は行わなかった。結果を表 3 に示す。

表 3 の (A) 行では、計算量を一定に保ちながら、アテンションヘッドの数とアテンションキーと値の次元を変化させている。単一ヘッドのアテンションは、最良の設定よりも 0.9 BLEU 低いが、ヘッドが多すぎると品質も低下する。

表 3 の (B) 行では、アテンションキーサイズ $d_k$ を小さくするとモデルの品質が低下することがわかる。これは、互換性を判断するのが容易ではなく、ドット積よりも高度な互換性関数が有益である可能性を示唆している。さらに、(C) 行と (D) 行では、予想どおり、大規模なモデルの方が優れており、ドロップアウトは過適合を回避するのに非常に役立つことがわかる。(E) 行では、正弦波の位置エンコーディングを学習された位置エンコーディングに置き換えており、ベースモデルとほぼ同じ結果が得られている。

### 6.3 英語の構成素解析

Transformer が他のタスクにも一般化できるかどうかを評価するために、英語の構成素解析に関する実験を行った。このタスクは、出力が強い構造制約を受け、入力よりも大幅に長くなるという課題がある。さらに、RNN のシーケンス・ツー・シーケンスモデルは、小規模データ体制では最先端の結果を達成できていない。

Penn Treebank [25] のウォールストリートジャーナル (WSJ) 部分（約 40K の学習文）で $d_{model}$ = 1024 の 4 層 Transformer を学習した。約 17M の文を含む、より大規模な高信頼度および BerkeleyParser コーパスを使用した半教師あり設定でも学習した。WSJ のみの設定では 16K のトークン、半教師あり設定では 32K のトークンの語彙を使用した。

セクション 22 の開発セットでドロップアウト、アテンションと残差（セクション 5.4）、学習率、ビームサイズを選択するために少数の実験のみを行い、他のすべてのパラメータは英語-ドイツ語のベース翻訳モデルから変更しなかった。推論中は、最大出力長を入力長 + 300 に増やした。WSJ のみと半教師あり設定の両方で、ビームサイズ 21、 $\alpha$ = 0.3 を使用した。

表 4 の結果は、タスク固有の調整がないにもかかわらず、提案モデルが驚くほど優れた性能を発揮し、Recurrent Neural Network Grammar [8] を除く、以前に報告されたすべてのモデルよりも優れた結果が得られていることを示している。

RNN シーケンス・ツー・シーケンスモデル [37] とは対照的に、Transformer は、40K 文の WSJ 学習セットのみで学習した場合でも、BerkeleyParser [29] を上回っている。

## 7 Conclusion

本研究では、Transformerという、注意機構のみに基づいた初の系列変換モデルを提示した。このモデルは、エンコーダ・デコーダ構造において最も一般的に用いられる再帰層を、多頭自己注意に置き換えるものである。

翻訳タスクにおいて、Transformerは再帰層や畳み込み層に基づく構造よりも大幅に高速に学習できる。WMT 2014の英語-ドイツ語および英語-フランス語翻訳タスクの両方で、新たな最高性能を達成した。特に、英語-ドイツ語翻訳タスクでは、既存のアンサンブルモデルをも凌駕する性能を示した。

著者らは、注意機構に基づくモデルの将来性に期待しており、他のタスクへの応用も計画している。テキスト以外の入出力モダリティを含む問題や、画像、音声、動画などの大規模な入出力を効率的に処理するための局所的、制限付き注意機構の調査も予定している。また、生成の逐次性を低減することも、著者らの研究目標の一つである。

## Acknowledgements

本研究に対し、Nal KalchbrennerとStephan Gouwsから有益なコメント、修正、およびインスピレーションを得たことに感謝する。

## References

以下に、論文の参考文献を日本語で要約する。

*   **[1] Ba, Kiros, & Hinton (2016):** 層正規化に関する論文。
*   **[2] Bahdanau, Cho, & Bengio (2014):** ニューラル機械翻訳におけるアライメントと翻訳の同時学習に関する論文。
*   **[3] Britz, Goldie, Luong, & Le (2017):** ニューラル機械翻訳アーキテクチャの大規模探索に関する論文。
*   **[4] Cheng, Dong, & Lapata (2016):** 機械読解のためのLSTMネットワークに関する論文。
*   **[5] Cho, van Merrienboer, Gulcehre, Bougares, Schwenk, & Bengio (2014):** 統計的機械翻訳のためのRNNエンコーダ・デコーダを用いたフレーズ表現学習に関する論文。
*  **[6] Chollet (2016):** 深層分離畳み込みを用いたXceptionに関する論文。
*   **[7] Chung, Gulcehre, Cho, & Bengio (2014):** ゲート付きリカレントニューラルネットワークの系列モデリングに関する実証的評価に関する論文。
*  **[8] Dyer, Kuncoro, Ballesteros, & Smith (2016):** リカレントニューラルネットワーク文法に関する論文。
*   **[9] Gehring, Auli, Grangier, Yarats, & Dauphin (2017):** 畳み込みを用いた系列から系列への学習に関する論文。
*   **[10] Graves (2013):** リカレントニューラルネットワークを用いた系列生成に関する論文。
*   **[11] He, Zhang, Ren, & Sun (2016):** 画像認識のための深層残差学習に関する論文。
*   **[12] Hochreiter, Bengio, Frasconi, & Schmidhuber (2001):** リカレントネットワークにおける勾配の流れと長期依存関係の学習の困難さに関する論文。
*   **[13] Hochreiter & Schmidhuber (1997):** 長短期記憶に関する論文。
*   **[14] Huang & Harper (2009):** 潜在アノテーションを用いた自己学習PCFG文法に関する論文。
*   **[15] Jozefowicz, Vinyals, Schuster, Shazeer, & Wu (2016):** 言語モデリングの限界を探る論文。
*   **[16] Kaiser & Bengio (2016):** アクティブメモリが注意機構の代替となるかに関する論文。
*   **[17] Kaiser & Sutskever (2016):** ニューラルGPUがアルゴリズムを学習する論文。
*   **[18] Kalchbrenner, Espeholt, Simonyan, van den Oord, Graves, & Kavukcuoglu (2017):** 線形時間でのニューラル機械翻訳に関する論文。
*   **[19] Kim, Denton, Hoang, & Rush (2017):** 構造化注意ネットワークに関する論文。
*   **[20] Kingma & Ba (2015):** 確率的最適化のためのAdam法に関する論文。
*   **[21] Kuchaiev & Ginsburg (2017):** LSTMネットワークの因数分解に関する論文。
*   **[22] Lin, Feng, dos Santos, Yu, Xiang, Zhou, & Bengio (2017):** 構造化自己注意文埋め込みに関する論文。
*   **[23] Luong, Le, Sutskever, Vinyals, & Kaiser (2015):** マルチタスク系列から系列への学習に関する論文。
*   **[24] Luong, Pham, & Manning (2015):** 注意機構に基づくニューラル機械翻訳の効果的なアプローチに関する論文。
*   **[25] Marcus, Marcinkiewicz, & Santorini (1993):** 大規模な英語アノテーション付きコーパス（ペンツリーバンク）の構築に関する論文。
*   **[26] McClosky, Charniak, & Johnson (2006):** 構文解析のための効果的な自己学習に関する論文。
*   **[27] Parikh, Täckström, Das, & Uszkoreit (2016):** 分解可能な注意モデルに関する論文。
*   **[28] Paulus, Xiong, & Socher (2017):** 抽象的な要約のための深層強化学習モデルに関する論文。
*   **[29] Petrov, Barrett, Thibaux, & Klein (2006):** 正確でコンパクトかつ解釈可能なツリーアノテーションの学習に関する論文。
*   **[30] Press & Wolf (2016):** 言語モデルを改善するための出力埋め込みの使用に関する論文。
*   **[31] Sennrich, Haddow, & Birch (2015):** サブワードユニットを用いた希少語のニューラル機械翻訳に関する論文。
*   **[32] Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, & Dean (2017):** 非常に大規模なニューラルネットワーク：スパースゲート混合エキスパート層に関する論文。
*   **[33] Srivastava, Hinton, Krizhevsky, Sutskever, & Salakhutdinov (2014):** ドロップアウト：ニューラルネットワークの過学習を防ぐための簡単な方法に関する論文。
*   **[34] Sukhbaatar, Szlam, Weston, & Fergus (2015):** エンドツーエンドメモリネットワークに関する論文。
*   **[35] Sutskever, Vinyals, & Le (2014):** ニューラルネットワークを用いた系列から系列への学習に関する論文。
*   **[36] Szegedy, Vanhoucke, Ioffe, Shlens, & Wojna (2015):** コンピュータビジョンのためのインセプションアーキテクチャの再考に関する論文。
*   **[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, & Hinton (2015):** 外国語としての文法に関する論文。
*   **[38] Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao, Gao, Macherey, et al. (2016):** Googleのニューラル機械翻訳システム：人間と機械翻訳のギャップを埋める論文。
*   **[39] Zhou, Cao, Wang, Li, & Xu (2016):** ニューラル機械翻訳のための高速順方向接続を備えた深層リカレントモデルに関する論文。
*   **[40] Zhu, Zhang, Chen, Zhang, & Zhu (2013):** 高速かつ正確なシフト削減構成要素解析に関する論文。

## Attention Visualizations

この論文では、Transformerモデルにおける注意機構の可視化について述べている。具体的には、エンコーダの自己注意層における注意の様子をいくつかの例を通して示している。

図3では、層5のエンコーダ自己注意において、長距離依存関係を捉えている例を示している。特に、「making...more difficult」というフレーズを完成させるために、動詞「making」が遠くの単語に注意を向けている様子が示されている。異なる色は異なる注意ヘッドを表している。

図4では、層5の2つの注意ヘッドが照応解析に関与している例を示している。上図はヘッド5の注意全体を、下図はヘッド5と6の単語「its」からの注意をそれぞれ示している。この図から、注意が特定の単語に集中していることがわかる。

図5では、文の構造に関連する挙動を示す注意ヘッドの例を示している。異なるヘッドが異なるタスクを学習していることがわかる。
