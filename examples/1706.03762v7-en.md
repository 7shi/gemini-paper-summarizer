# Abstract

The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

# Overview

This paper introduces the Transformer, a novel neural network architecture relying solely on attention mechanisms, which achieves state-of-the-art results in machine translation and constituency parsing while being more parallelizable and requiring less training time compared to recurrent or convolutional models.

## Problem Statement

The paper addresses the limitations of recurrent and convolutional neural networks in sequence transduction tasks, specifically their sequential computation nature which hinders parallelization and makes it difficult to learn long-range dependencies. It proposes a new architecture, the Transformer, that relies solely on attention mechanisms to overcome these limitations.

## Methodology

The paper proposes a novel neural network architecture called the Transformer. This architecture relies entirely on attention mechanisms to model dependencies between input and output sequences, dispensing with recurrence and convolutions. The Transformer employs multi-headed self-attention to draw global dependencies between input and output. It also uses positional encodings to incorporate information about the order of tokens in a sequence. The model is composed of an encoder and a decoder, each consisting of a stack of identical layers. Each layer contains multi-head self-attention and a position-wise feed-forward network.

## Novelty

The paper introduces the Transformer, a novel neural network architecture for sequence transduction tasks that relies solely on attention mechanisms, dispensing with recurrence and convolutions. This is a departure from the dominant sequence transduction models that are based on complex recurrent or convolutional neural networks, which typically include an encoder and a decoder connected through an attention mechanism.

# Chapter Structure

- 1 Introduction
- 2 Background
- 3 Model Architecture
  - 3.1 Encoder and Decoder Stacks
  - 3.2 Attention
    - 3.2.1 Scaled Dot-Product Attention
    - 3.2.2 Multi-Head Attention
    - 3.2.3 Applications of Attention in our Model
  - 3.3 Position-wise Feed-Forward Networks
  - 3.4 Embeddings and Softmax
  - 3.5 Positional Encoding
- 4 Why Self-Attention
- 5 Training
  - 5.1 Training Data and Batching
  - 5.2 Hardware and Schedule
  - 5.3 Optimizer
  - 5.4 Regularization
- 6 Results
  - 6.1 Machine Translation
  - 6.2 Model Variations
  - 6.3 English Constituency Parsing
- 7 Conclusion
- Acknowledgements
- References
- Attention Visualizations

## 1 Introduction

Recurrent neural networks (RNNs), particularly Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), are the dominant approach for sequence modeling and transduction tasks like language modeling and machine translation. These models process input and output sequences sequentially, generating hidden states $h_t$ based on the previous hidden state $h_{t-1}$ and the current input at position $t$. This sequential nature hinders parallelization within training examples, especially with longer sequences. Although techniques like factorization and conditional computation have improved computational efficiency, the fundamental constraint of sequential computation remains. Attention mechanisms have become essential in sequence modeling, allowing the modeling of dependencies regardless of distance in the input or output sequences. However, these mechanisms are typically used with recurrent networks. This paper introduces the Transformer, a model architecture that eliminates recurrence and relies solely on attention mechanisms to capture global dependencies between input and output. The Transformer allows for significantly more parallelization and achieves state-of-the-art translation quality with reduced training time.

## 2 Background

Section 2, "Background," discusses the motivation for reducing sequential computation in sequence modeling tasks, highlighting the limitations of recurrent and convolutional neural networks. While recurrent models process input sequentially, hindering parallelization, convolutional models like the Extended Neural GPU, ByteNet, and ConvS2S offer parallel computation but struggle with long-range dependencies due to the increasing number of operations required with distance. The Transformer model addresses this by using a constant number of operations, mitigating the issue of long-range dependencies, albeit with reduced resolution that is addressed using Multi-Head Attention. The section also introduces self-attention, which relates different positions within a single sequence, and its successful applications in various NLP tasks. Finally, it emphasizes that the Transformer is unique as the first transduction model that relies entirely on self-attention, without using sequence-aligned RNNs or convolutions.

## 3 Model Architecture

The Transformer model architecture employs an encoder-decoder structure. The encoder maps an input sequence of symbol representations $(x_1,..., x_n)$ to a sequence of continuous representations $z = (z_1,..., z_n)$. The decoder then generates an output sequence $(y_1,..., y_m)$ one element at a time, auto-regressively.

The encoder consists of $N=6$ identical layers, each with two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Residual connections are employed around each sub-layer, followed by layer normalization. The decoder also has $N=6$ identical layers, with an additional multi-head attention sub-layer over the encoder output. A masking mechanism in the decoder's self-attention sub-layer prevents attending to subsequent positions, preserving the auto-regressive property.

The attention mechanism maps a query and a set of key-value pairs to an output. Scaled Dot-Product Attention computes the dot products of the query with all keys, scaled by $\frac{1}{\sqrt{d_k}}$, followed by a softmax function to obtain weights on the values. Multi-Head Attention projects the queries, keys, and values $h$ times with different learned linear projections and performs the attention function in parallel, concatenating the results.

Multi-head attention is used in three ways: "encoder-decoder attention" where the queries come from the previous decoder layer, and the keys and values come from the encoder output; self-attention layers in the encoder where all keys, values, and queries come from the previous encoder layer; and self-attention layers in the decoder where all keys, values, and queries come from the previous decoder layer, with masking.

Each layer in the encoder and decoder also contains a position-wise feed-forward network consisting of two linear transformations with a ReLU activation in between. The input and output dimensionality is $d_{model} = 512$, and the inner-layer dimensionality is $d_{ff} = 2048$.

Learned embeddings are used to convert input and output tokens to vectors of dimension $d_{model}$. The same weight matrix is shared between the two embedding layers and the pre-softmax linear transformation. In the embedding layers, weights are multiplied by $\sqrt{d_{model}}$.

Positional encodings are added to the input embeddings to provide information about the relative or absolute position of the tokens in the sequence. Sine and cosine functions of different frequencies are used for this purpose:
$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$
$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$
where $pos$ is the position and $i$ is the dimension.

## 4 Why Self-Attention

Section 4, "Why Self-Attention," compares self-attention layers to recurrent and convolutional layers for mapping a variable-length sequence of symbol representations to another sequence of equal length. The comparison is based on three criteria: total computational complexity per layer, the amount of computation that can be parallelized (measured by the minimum number of sequential operations), and the path length between long-range dependencies in the network.

Self-attention layers connect all positions with a constant number of sequentially executed operations, while recurrent layers require $O(n)$ sequential operations, where $n$ is the sequence length. Self-attention layers are faster than recurrent layers when $n$ is smaller than the representation dimensionality $d$. Convolutional layers require a stack of $O(n/k)$ layers with contiguous kernels or $O(log_k(n))$ with dilated kernels to connect all pairs of input and output positions, increasing the length of the longest paths. Self-attention can be restricted to a neighborhood of size $r$ around the output position, increasing the maximum path length to $O(n/r)$.

The section concludes that self-attention layers are more efficient than recurrent layers when $n < d$ and provide shorter paths for long-range dependencies than convolutional layers. These advantages motivate the use of self-attention in the Transformer model.

## 5 Training

Section 5 details the training procedures for the Transformer models.

**5.1 Training Data and Batching:** The models were trained on the WMT 2014 English-German dataset, which contains approximately 4.5 million sentence pairs. Byte-pair encoding was used to encode the sentences, resulting in a shared vocabulary of around 37,000 tokens. For English-French translation, the larger WMT 2014 dataset with 36 million sentences was used, with a 32,000 word-piece vocabulary. Sentence pairs were grouped into batches based on approximate sequence length, with each batch containing roughly 25,000 source and 25,000 target tokens.

**5.2 Hardware and Schedule:** Training was conducted on a machine equipped with 8 NVIDIA P100 GPUs. The base models, using the hyperparameters described in the paper, took about 0.4 seconds per training step and were trained for 100,000 steps (12 hours). The larger models required 1.0 seconds per step and were trained for 300,000 steps (3.5 days).

**5.3 Optimizer:** The Adam optimizer was used with parameters $\beta_1 = 0.9$, $\beta_2 = 0.98$, and $\epsilon = 10^{-9}$. The learning rate was varied during training according to the formula:

$lrate = d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})$

The learning rate increased linearly for the first $warmup\_steps$ training steps and then decreased proportionally to the inverse square root of the step number. $warmup\_steps$ was set to 4000.

**5.4 Regularization:** Three types of regularization were used during training:
1.  **Residual Dropout:** Dropout was applied to the output of each sub-layer before it was added to the sub-layer input and normalized. Dropout was also applied to the sums of the embeddings and positional encodings in both the encoder and decoder stacks. The base model used a dropout rate of $p_{drop} = 0.1$.
2.  **Label Smoothing:** Label smoothing with a value of $\epsilon_{ls} = 0.1$ was employed during training, which improved accuracy and BLEU score, although it hurt perplexity.

## 6 Results

Section 6 presents the experimental results of the proposed Transformer model.

In Section 6.1, the Transformer model's performance on machine translation tasks is evaluated. The "big" Transformer model achieves a new state-of-the-art BLEU score of 28.4 on the WMT 2014 English-to-German translation task, surpassing previous models, including ensembles, by over 2.0 BLEU. The base model also outperforms all previously published models at a fraction of the training cost. On the WMT 2014 English-to-French translation task, the big model achieves a BLEU score of 41.8, outperforming previous single models, with less than 1/4 of the training cost of the previous state-of-the-art model.

Section 6.2 explores the importance of different components of the Transformer through model variations. The number of attention heads and the attention key and value dimensions are varied, while keeping computation constant. It is found that single-head attention performs worse, and quality also drops with too many heads. Reducing the attention key size also hurts model quality. Larger models perform better, and dropout is helpful in avoiding over-fitting. Replacing sinusoidal positional encodings with learned positional embeddings yields similar results.

Section 6.3 presents results on English constituency parsing. The Transformer, despite not being specifically tuned for this task, performs surprisingly well, yielding better results than previously reported models, with the exception of the Recurrent Neural Network Grammar. The Transformer outperforms the Berkeley-Parser even when trained only on the WSJ training set of 40K sentences.

## 7 Conclusion

The Transformer model, which relies solely on attention mechanisms, is introduced as an alternative to recurrent layers in sequence transduction tasks.  It achieves state-of-the-art results in English-to-German and English-to-French translation, surpassing previous models and ensembles. The authors express enthusiasm for future applications of attention-based models to diverse tasks and modalities, including images, audio, and video, and aim to reduce sequential generation.

## Acknowledgements

The authors express their gratitude to Nal Kalchbrenner and Stephan Gouws for their helpful feedback, corrections, and inspiration.

## References

This section provides a comprehensive list of references, including publications on topics such as layer normalization, neural machine translation, recurrent neural networks, attention mechanisms, convolutional networks, and sequence-to-sequence learning. It also includes references to specific models and techniques such as the Extended Neural GPU, ByteNet, ConvS2S, and Transformer, as well as work on positional embeddings, dropout, and label smoothing. Additionally, there are references to work on parsing, summarization, and other natural language processing tasks. The references span a range of publication venues, including conferences like NeurIPS, ICLR, ACL, and EMNLP, as well as arXiv preprints.

## Attention Visualizations

The "Attention Visualizations" section presents visualizations of the attention mechanisms within the Transformer model's encoder. Figure 3 illustrates how attention heads can capture long-range dependencies, specifically focusing on the verb "making" and its connection to "more difficult". Figures 4 and 5 show how different attention heads focus on different parts of the input, indicating that the heads learn to perform different tasks. Figure 4 shows attention heads apparently involved in anaphora resolution. Figure 5 shows how attention heads exhibit behavior related to the structure of the sentence.
