# Titel

Aufmerksamkeit ist alles, was Sie brauchen.

# Abstract

Die vorherrschenden Modelle für die Sequenztransduktion basieren auf komplexen rekurrenten oder Faltungs-Neuronalen Netzen, die einen Encoder und einen Decoder umfassen. Die leistungsstärksten Modelle verbinden den Encoder und Decoder zusätzlich durch einen Aufmerksamkeitsmechanismus. Wir schlagen eine neue, einfache Netzwerkarchitektur vor, den Transformer, der ausschließlich auf Aufmerksamkeitsmechanismen basiert und vollständig auf Rekurrenzen und Faltungen verzichtet. Experimente mit zwei maschinellen Übersetzungsaufgaben zeigen, dass diese Modelle eine höhere Qualität aufweisen, gleichzeitig besser parallelisierbar sind und deutlich weniger Trainingszeit benötigen. Unser Modell erreicht 28,4 BLEU bei der WMT 2014 Englisch-Deutsch Übersetzungsaufgabe und verbessert damit die bisherigen besten Ergebnisse, einschließlich Ensembles, um über 2 BLEU. Bei der WMT 2014 Englisch-Französisch Übersetzungsaufgabe erreicht unser Modell einen neuen Single-Model State-of-the-Art BLEU Score von 41,8 nach einer Trainingszeit von 3,5 Tagen auf acht GPUs, was einen kleinen Bruchteil der Trainingskosten der besten Modelle aus der Literatur darstellt. Wir zeigen, dass der Transformer gut auf andere Aufgaben generalisiert, indem wir ihn erfolgreich auf die englische Konstituentenanalyse mit großen und begrenzten Trainingsdaten anwenden.

# Zusammenfassung

Das Paper präsentiert den Transformer, eine neuartige Netzwerkarchitektur für Sequenztransduktionsaufgaben, die ausschließlich auf Aufmerksamkeitsmechanismen basiert und im Vergleich zu bestehenden Modellen überlegene Ergebnisse in der maschinellen Übersetzung erzielt, während sie gleichzeitig eine höhere Parallelisierbarkeit und geringere Trainingszeiten aufweist.

## Problemstellung

Das Dokument präsentiert eine neue Netzwerkarchitektur namens Transformer, die auf rekurrente und konvolutionale neuronale Netze verzichtet und stattdessen ausschließlich auf Aufmerksamkeitsmechanismen basiert, um Abhängigkeiten zwischen Eingabe- und Ausgabesequenzen zu modellieren. Das Hauptproblem, das angegangen wird, ist die Einschränkung der Parallelisierbarkeit und der damit verbundenen Trainingszeiten bei sequentiellen Modellen, insbesondere bei längeren Sequenzen. Die Arbeit zielt darauf ab, eine effizientere und parallelisierbare Alternative zu bestehenden Architekturen für Sequenz-Transduktionsaufgaben wie maschinelle Übersetzung zu schaffen.

## Methodik

Das Dokument schlägt eine neuartige Netzwerkarchitektur namens Transformer vor, die ausschließlich auf Aufmerksamkeitsmechanismen (Attention Mechanisms) basiert und auf Rekurrenzen und Faltungen (Convolutions) verzichtet. Der Transformer verwendet gestapelte Self-Attention- und Point-Wise-Fully-Connected-Schichten für Encoder und Decoder. Die Aufmerksamkeit wird als eine Funktion definiert, die eine Abfrage (Query) und eine Menge von Schlüssel-Wert-Paaren auf eine Ausgabe abbildet. Die Ausgabe wird als gewichtete Summe der Werte berechnet, wobei die Gewichte durch eine Kompatibilitätsfunktion der Abfrage mit dem entsprechenden Schlüssel bestimmt werden. Im Speziellen wird die "Scaled Dot-Product Attention" verwendet, bei der die Dot-Produkte mit $1/\sqrt{d_k}$ skaliert werden, wobei $d_k$ die Dimension der Schlüssel ist. Um die Aufmerksamkeit auf verschiedene Repräsentationsräume zu richten, wird Multi-Head-Attention verwendet. Dabei werden die Abfragen, Schlüssel und Werte linear projiziert und die Aufmerksamkeit parallel auf den projizierten Versionen berechnet. Die Ausgaben werden dann konkateniert und erneut projiziert. Zusätzlich zu den Aufmerksamkeits-Subschichten enthält jede Schicht im Encoder und Decoder ein Position-Wise-Feed-Forward-Netzwerk, das auf jede Position separat angewendet wird. Um die Reihenfolge der Sequenz zu berücksichtigen, werden Positionskodierungen zu den Eingebettungen addiert. Der Transformer verwendet drei Arten von Multi-Head-Attention: Encoder-Decoder-Attention, Encoder-Self-Attention und Decoder-Self-Attention, wobei letztere maskiert ist, um die autoregressive Eigenschaft zu erhalten.

## Neuartigkeit

Das Dokument präsentiert eine neuartige Netzwerkarchitektur namens Transformer für Sequenztransduktionsaufgaben, die vollständig auf Aufmerksamkeitsmechanismen basiert und auf Rekurrenzen und Faltungen verzichtet. Es zeigt, dass dieses Modell in der Maschinenübersetzung überlegene Ergebnisse erzielt, während es gleichzeitig besser parallelisierbar ist und weniger Trainingszeit benötigt.

# Dokumentenstruktur

- 1 Introduction
- 2 Background
- 3 Model Architecture
  - 3.1 Encoder and Decoder Stacks
  - 3.2 Attention
    - 3.2.1 Scaled Dot-Product Attention
    - 3.2.2 Multi-Head Attention
    - 3.2.3 Applications of Attention in our Model
  - 3.3 Position-wise Feed-Forward Networks
  - 3.4 Embeddings and Softmax
  - 3.5 Positional Encoding
- 4 Why Self-Attention
- 5 Training
  - 5.1 Training Data and Batching
  - 5.2 Hardware and Schedule
  - 5.3 Optimizer
  - 5.4 Regularization
- 6 Results
  - 6.1 Machine Translation
  - 6.2 Model Variations
  - 6.3 English Constituency Parsing
- 7 Conclusion
- Acknowledgements
- References
- Attention Visualizations

## 1 Introduction

Rekurrente neuronale Netze, insbesondere LSTM- und GRU-Netze, haben sich als State-of-the-Art für Sequenzmodellierungs- und Transduktionsaufgaben etabliert. Diese Modelle faktorisieren die Berechnung entlang der Symbolpositionen der Eingabe- und Ausgabesequenzen und erzeugen eine Sequenz von verborgenen Zuständen $h_t$, die von dem vorherigen verborgenen Zustand $h_{t-1}$ und der Eingabe für Position $t$ abhängen. Diese inhärent sequenzielle Natur verhindert die Parallelisierung innerhalb von Trainingsbeispielen, was bei längeren Sequenzen kritisch wird. Obwohl jüngste Arbeiten die Recheneffizienz durch Faktorisierungstricks und bedingte Berechnungen verbessert haben, bleibt die grundlegende Einschränkung der sequenziellen Berechnung bestehen. Aufmerksamkeitsmechanismen sind zu einem integralen Bestandteil von Sequenzmodellierungs- und Transduktionsmodellen geworden, die die Modellierung von Abhängigkeiten ohne Rücksicht auf deren Abstand in den Eingabe- oder Ausgabesequenzen ermöglichen. In dieser Arbeit wird der Transformer vorgestellt, eine Modellarchitektur, die auf Rekurrenzen verzichtet und sich stattdessen vollständig auf einen Aufmerksamkeitsmechanismus stützt, um globale Abhängigkeiten zwischen Eingabe und Ausgabe zu ziehen. Der Transformer ermöglicht eine signifikant höhere Parallelisierung und kann nach nur zwölf Stunden Training auf acht P100-GPUs einen neuen Stand der Technik in der Übersetzungsqualität erreichen.

## 2 Background

Der Abschnitt "2 Hintergrund" des Artikels diskutiert die Motivation für die Entwicklung des Transformer-Modells, insbesondere im Hinblick auf die Reduzierung sequentieller Berechnungen. Er beginnt mit der Erwähnung von Modellen wie dem Extended Neural GPU, ByteNet und ConvS2S, die alle auf Convolutional Neural Networks (CNNs) basieren und parallele Berechnungen für alle Eingabe- und Ausgabepositionen ermöglichen. Diese Modelle haben jedoch das Problem, dass die Anzahl der Operationen, die erforderlich sind, um Signale zwischen zwei Positionen zu beziehen, mit der Distanz zwischen diesen Positionen wächst. Dies erschwert das Erlernen von Abhängigkeiten zwischen weit entfernten Positionen. Im Gegensatz dazu reduziert der Transformer dies auf eine konstante Anzahl von Operationen, allerdings auf Kosten einer reduzierten effektiven Auflösung. Dies wird durch Multi-Head Attention kompensiert. Der Abschnitt führt auch das Konzept der Self-Attention ein, die verschiedene Positionen einer einzelnen Sequenz in Beziehung setzt, um eine Repräsentation der Sequenz zu berechnen. Schließlich wird festgestellt, dass der Transformer das erste Transduktionsmodell ist, das vollständig auf Self-Attention basiert und auf sequenzielle RNNs oder Convolutionen verzichtet.

## 3 Model Architecture

Das Transformer-Modell, das in dieser Arbeit vorgestellt wird, verwendet eine Encoder-Decoder-Struktur.

**3.1 Encoder und Decoder Stacks**

Der Encoder besteht aus $N=6$ identischen Schichten, wobei jede Schicht zwei Unterschichten hat: eine Multi-Head-Self-Attention-Schicht und eine Position-weise Fully-Connected Feed-Forward-Netzwerk. Um jede dieser Unterschichten wird eine Residualverbindung mit anschließender Layer-Normalisierung angewendet: $LayerNorm(x + Sublayer(x))$. Alle Unterschichten und Einbettungsschichten produzieren Ausgaben der Dimension $d_{model} = 512$.

Der Decoder besteht ebenfalls aus $N=6$ identischen Schichten. Zusätzlich zu den zwei Unterschichten des Encoders enthält der Decoder eine dritte Unterschicht, die Multi-Head-Attention über die Ausgabe des Encoder-Stacks ausführt. Wie beim Encoder werden Residualverbindungen und Layer-Normalisierung verwendet. Die Self-Attention-Unterschicht des Decoders wird modifiziert, um zu verhindern, dass Positionen auf nachfolgende Positionen aufmerksam werden. Dies wird durch Maskierung erreicht, die in Kombination mit der Verschiebung der Ausgabeeinbettungen um eine Position sicherstellt, dass die Vorhersagen für Position $i$ nur von bekannten Ausgaben an Positionen kleiner als $i$ abhängen.

**3.2 Aufmerksamkeit (Attention)**

Die Aufmerksamkeitsfunktion bildet eine Query und eine Menge von Key-Value-Paaren auf eine Ausgabe ab, wobei Query, Keys, Values und die Ausgabe Vektoren sind. Die Ausgabe wird als gewichtete Summe der Values berechnet, wobei die Gewichte durch eine Kompatibilitätsfunktion der Query mit dem entsprechenden Key bestimmt werden.

**3.2.1 Scaled Dot-Product Attention**

Die verwendete Aufmerksamkeitsfunktion wird als "Scaled Dot-Product Attention" bezeichnet. Die Eingabe besteht aus Queries und Keys der Dimension $d_k$ und Values der Dimension $d_v$. Die Dot-Produkte der Query mit allen Keys werden berechnet, durch $\sqrt{d_k}$ dividiert und eine Softmax-Funktion angewendet, um die Gewichte der Values zu erhalten. Die Berechnung erfolgt als:
$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

**3.2.2 Multi-Head Attention**

Anstelle einer einzelnen Aufmerksamkeitsfunktion mit $d_{model}$-dimensionalen Keys, Values und Queries werden diese $h$-mal mit verschiedenen gelernten linearen Projektionen auf $d_k$, $d_k$ bzw. $d_v$ projiziert. Auf jede dieser projizierten Versionen wird die Aufmerksamkeitsfunktion parallel angewendet, was zu $d_v$-dimensionalen Ausgaben führt. Diese werden konkateniert und erneut projiziert, um die endgültige Ausgabe zu erhalten. Dies ermöglicht dem Modell, Informationen aus verschiedenen Darstellungsunterräumen zu beachten.

$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$
wobei $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$

Es werden $h=8$ parallele Aufmerksamkeits-Schichten verwendet, mit $d_k = d_v = d_{model}/h = 64$.

**3.2.3 Anwendungen der Aufmerksamkeit im Modell**

Das Transformer-Modell verwendet Multi-Head-Attention auf drei Arten:
*   In den "Encoder-Decoder-Attention"-Schichten stammen die Queries aus der vorherigen Decoder-Schicht, während die Keys und Values aus der Ausgabe des Encoders stammen.
*   Der Encoder enthält Self-Attention-Schichten, in denen Keys, Values und Queries aus der Ausgabe der vorherigen Schicht des Encoders stammen.
*   Die Self-Attention-Schichten des Decoders erlauben es jeder Position im Decoder, auf alle Positionen im Decoder bis einschließlich dieser Position aufmerksam zu werden. Eine Maskierung verhindert dabei die Informationsfluss von rechts nach links, um die autoregressive Eigenschaft zu erhalten.

**3.3 Position-wise Feed-Forward Networks**

Zusätzlich zu den Attention-Unterschichten enthält jede Schicht des Encoders und Decoders ein Position-weises Feed-Forward-Netzwerk, das auf jede Position separat und identisch angewendet wird. Es besteht aus zwei linearen Transformationen mit einer ReLU-Aktivierung dazwischen:
$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$
Die Dimensionalität von Input und Output ist $d_{model}=512$, die innere Schicht hat die Dimensionalität $d_{ff}=2048$.

**3.4 Einbettungen und Softmax**

Wie bei anderen Sequenz-Transduktionsmodellen werden gelernte Einbettungen verwendet, um Eingabe- und Ausgabetoken in Vektoren der Dimension $d_{model}$ zu konvertieren. Eine lineare Transformation und Softmax-Funktion konvertiert die Decoder-Ausgabe in Vorhersagen für das nächste Token. Die Gewichtsmatrix zwischen den Einbettungsschichten und der Pre-Softmax-linearen Transformation wird geteilt. In den Einbettungsschichten werden die Gewichte mit $\sqrt{d_{model}}$ multipliziert.

**3.5 Positionelle Kodierung**

Da das Modell keine Rekurrenz und Faltung verwendet, wird eine positionelle Kodierung zu den Eingabe-Einbettungen addiert, um Informationen über die relative oder absolute Position der Token in der Sequenz zu erhalten. Die positionelle Kodierung hat die gleiche Dimension $d_{model}$ wie die Einbettungen. Es werden Sinus- und Kosinusfunktionen unterschiedlicher Frequenzen verwendet:
$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$
$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$
Die Wellenlängen bilden eine geometrische Progression von $2\pi$ bis $10000 \cdot 2\pi$.

## 4 Why Self-Attention

In diesem Abschnitt werden verschiedene Aspekte von Self-Attention-Layern mit rekurrenten und konvolutionalen Layern verglichen, die üblicherweise zur Abbildung einer Sequenz von Symbolrepräsentationen $(x_1, ..., x_n)$ auf eine andere Sequenz gleicher Länge $(z_1, ..., z_n)$ verwendet werden, wobei $x_i, z_i \in \mathbb{R}^d$. Es werden drei Kriterien zur Motivation der Verwendung von Self-Attention betrachtet:

1.  Die gesamte Rechenkomplexität pro Layer.
2.  Das Ausmaß der Parallelisierbarkeit der Berechnung, gemessen an der minimalen Anzahl sequentieller Operationen.
3.  Die Pfadlänge zwischen Abhängigkeiten über große Distanzen im Netzwerk.

Self-Attention-Layer verbinden alle Positionen mit einer konstanten Anzahl sequentiell ausgeführter Operationen, während ein rekurrenter Layer $O(n)$ sequentielle Operationen benötigt. In Bezug auf die Rechenkomplexität sind Self-Attention-Layer schneller als rekurrente Layer, wenn die Sequenzlänge $n$ kleiner ist als die Repräsentationsdimensionalität $d$. Konvolutionale Layer mit Kernelbreite $k < n$ verbinden nicht alle Paare von Eingangs- und Ausgangspositionen, was zu einer erhöhten Pfadlänge führt. Self-Attention kann auch zu interpretierbareren Modellen führen, da einzelne Attention-Heads unterschiedliche Aufgaben erfüllen und Verhaltensweisen zeigen, die mit der syntaktischen und semantischen Struktur von Sätzen zusammenhängen.

## 5 Training

Der Abschnitt 5, "Training", beschreibt das Trainingsregime der Modelle.

**5.1 Trainingsdaten und Batching:** Die Modelle wurden auf dem WMT 2014 Englisch-Deutsch Datensatz mit ca. 4,5 Millionen Satzpaaren trainiert. Die Sätze wurden mit Byte-Pair-Encoding (BPE) kodiert, wobei ein gemeinsamer Vokabular von ca. 37.000 Token für die Quell- und Zielsprache verwendet wurde. Für Englisch-Französisch wurde ein größerer Datensatz mit 36 Millionen Sätzen und einem 32.000 Wort-Vokabular verwendet. Die Sätze wurden nach approximativer Sequenzlänge in Batches zusammengefasst, wobei jeder Batch ca. 25.000 Quell- und Zieltoken enthielt.

**5.2 Hardware und Zeitplan:** Die Modelle wurden auf einer Maschine mit 8 NVIDIA P100 GPUs trainiert. Die Basismodelle benötigten ca. 0,4 Sekunden pro Trainingsschritt und wurden insgesamt 100.000 Schritte lang (12 Stunden) trainiert. Die großen Modelle benötigten 1,0 Sekunden pro Schritt und wurden 300.000 Schritte lang (3,5 Tage) trainiert.

**5.3 Optimierer:** Der Adam-Optimierer wurde mit $\beta_1 = 0.9$, $\beta_2 = 0.98$ und $\epsilon = 10^{-9}$ verwendet. Die Lernrate wurde während des Trainings gemäß der Formel
$lrate = d_{model}^{-0.5} \cdot \min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})$
variiert. Die Lernrate wurde linear für die ersten $warmup\_steps$ Trainingsschritte erhöht und danach proportional zur inversen Quadratwurzel der Schrittnummer verringert. Es wurden $warmup\_steps = 4000$ verwendet.

**5.4 Regularisierung:** Während des Trainings wurden drei Arten der Regularisierung verwendet:
*   **Residual Dropout:** Dropout wurde auf die Ausgabe jedes Sub-Layers angewendet, bevor diese zum Sub-Layer-Input addiert und normalisiert wurde. Zusätzlich wurde Dropout auf die Summen der Embeddings und der Positionskodierungen angewendet. Die Dropout-Rate für das Basismodell betrug $p_{drop} = 0.1$.
*   **Label Smoothing:** Label Smoothing mit einem Wert von $\epsilon_{ls} = 0.1$ wurde verwendet. Dies verschlechterte zwar die Perplexität, verbesserte aber die Genauigkeit und den BLEU-Score.

## 6 Results

Abschnitt 6, "Ergebnisse", präsentiert die Resultate der durchgeführten Experimente.

**6.1 Maschinelle Übersetzung:**
In der maschinellen Übersetzung übertrifft das große Transformer-Modell auf dem WMT 2014 Englisch-Deutsch-Datensatz alle bisherigen Modelle (einschließlich Ensembles) um mehr als 2.0 BLEU-Punkte und erreicht einen neuen Bestwert von 28.4 BLEU. Das Training erfolgte auf 8 P100 GPUs innerhalb von 3.5 Tagen. Auch das Basismodell übertrifft alle bisher veröffentlichten Modelle und Ensembles zu einem Bruchteil der Trainingskosten. Auf dem WMT 2014 Englisch-Französisch-Datensatz erreicht das große Modell 41.0 BLEU und übertrifft damit alle bisherigen Einzelmodelle mit weniger als einem Viertel der Trainingskosten des vorherigen Bestmodells.

**6.2 Modellvariationen:**
Um die Bedeutung verschiedener Komponenten des Transformers zu evaluieren, wurden Variationen des Basismodells durchgeführt. Die Ergebnisse zeigen, dass die Anzahl der Aufmerksamkeitsköpfe und die Dimensionen der Schlüssel und Werte einen Einfluss auf die Qualität haben. Eine zu geringe oder zu hohe Anzahl von Köpfen führt zu Qualitätsverlusten. Die Reduzierung der Schlüsselgröße verschlechtert die Ergebnisse. Größere Modelle und Dropout führen zu besseren Ergebnissen. Das Ersetzen der sinusförmigen Positionskodierung durch gelernte Positionskodierungen führt zu ähnlichen Ergebnissen.

**6.3 Englische Konstituentenanalyse:**
Zur Evaluierung der Generalisierbarkeit des Transformers auf andere Aufgaben wurde die englische Konstituentenanalyse durchgeführt. Das Modell erreicht trotz fehlender aufgabenspezifischer Anpassung gute Ergebnisse und übertrifft alle bisherigen Modelle mit Ausnahme des Recurrent Neural Network Grammar. Im Vergleich zu RNN-basierten Modellen übertrifft der Transformer den Berkeley-Parser selbst beim Training auf dem WSJ-Trainingsdatensatz.

## 7 Conclusion

In dieser Arbeit wurde der Transformer vorgestellt, das erste Sequenztransduktionsmodell, das vollständig auf Aufmerksamkeit basiert. Es ersetzt die üblicherweise in Encoder-Decoder-Architekturen verwendeten rekurrenten Schichten durch mehrköpfige Selbstaufmerksamkeit.

Für Übersetzungsaufgaben kann der Transformer deutlich schneller trainiert werden als Architekturen, die auf rekurrenten oder Faltungsschichten basieren. Bei den Übersetzungsaufgaben WMT 2014 Englisch-Deutsch und WMT 2014 Englisch-Französisch wird ein neuer Stand der Technik erreicht. Bei der ersteren Aufgabe übertrifft das beste Modell sogar alle zuvor berichteten Ensembles.

Die Autoren sind von der Zukunft aufmerksamkeitsbasierter Modelle begeistert und planen, diese auf andere Aufgaben anzuwenden. Sie planen, den Transformer auf Probleme mit anderen Eingabe- und Ausgabemodalitäten als Text zu erweitern und lokale, eingeschränkte Aufmerksamkeitsmechanismen zu untersuchen, um große Eingaben und Ausgaben wie Bilder, Audio und Video effizient zu verarbeiten. Die weniger sequentielle Generierung ist ein weiteres Forschungsziel.

## Acknowledgements

Die Autoren bedanken sich bei Nal Kalchbrenner und Stephan Gouws für ihre hilfreichen Kommentare, Korrekturen und Inspiration.

## References

Die Referenzen umfassen eine Vielzahl von Arbeiten aus dem Bereich des maschinellen Lernens und der natürlichen Sprachverarbeitung. Viele der zitierten Arbeiten befassen sich mit neuronalen Netzen, insbesondere mit rekurrenten neuronalen Netzen (RNNs), Long Short-Term Memory (LSTM) Netzwerken und deren Anwendung in der Sequenzmodellierung und maschinellen Übersetzung. Es werden auch Arbeiten zu Aufmerksamkeitsmechanismen (Attention Mechanisms) und deren Integration in Encoder-Decoder-Architekturen zitiert. Einige Referenzen behandeln spezifische Techniken wie Layer Normalisierung, Dropout und Label Smoothing. Darüber hinaus werden Arbeiten zu Convolutional Neural Networks (CNNs) und deren Anwendung in der Sequenzmodellierung sowie zu verschiedenen Aspekten der neuronalen Maschinenübersetzung, einschließlich der Behandlung seltener Wörter und der Verwendung von Subwort-Einheiten, genannt. Die Referenzen enthalten auch Arbeiten zu Parsing-Methoden und zur Verwendung von Grammatiken als Fremdsprache. Schließlich werden auch Arbeiten zu Optimierungsmethoden wie Adam und zu verschiedenen Architekturen und Techniken im Bereich des maschinellen Lernens und der natürlichen Sprachverarbeitung zitiert.

## Attention Visualizations

In diesem Abschnitt werden Visualisierungen der Aufmerksamkeitsmechanismen des Transformer-Modells dargestellt. Abbildung 3 zeigt ein Beispiel für die Selbstaufmerksamkeit des Encoders in Schicht 5 von 6, wobei viele Aufmerksamkeitsköpfe eine entfernte Abhängigkeit des Verbs 'making' berücksichtigen. Abbildung 4 zeigt zwei Aufmerksamkeitsköpfe in Schicht 5 von 6, die anscheinend an der Auflösung von Anaphern beteiligt sind. Abbildung 5 verdeutlicht, dass verschiedene Aufmerksamkeitsköpfe unterschiedliche Aufgaben übernehmen. Die Aufmerksamkeit ist für bestimmte Wörter sehr scharf.
