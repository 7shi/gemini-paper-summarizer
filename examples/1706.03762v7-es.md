# Abstract

Los modelos dominantes de transducción de secuencias se basan en complejas redes neuronales recurrentes o convolucionales que incluyen un codificador y un decodificador. Los modelos con mejor rendimiento también conectan el codificador y el decodificador a través de un mecanismo de atención. Proponemos una nueva arquitectura de red simple, el Transformer, basada únicamente en mecanismos de atención, prescindiendo por completo de la recurrencia y las convoluciones. Los experimentos en dos tareas de traducción automática muestran que estos modelos son superiores en calidad, a la vez que son más paralelizables y requieren mucho menos tiempo para entrenar. Nuestro modelo alcanza 28.4 BLEU en la tarea de traducción de inglés a alemán WMT 2014, mejorando los mejores resultados existentes, incluyendo los conjuntos, en más de 2 BLEU. En la tarea de traducción de inglés a francés WMT 2014, nuestro modelo establece una nueva puntuación BLEU de vanguardia de un solo modelo de 41.8 después de entrenar durante 3.5 días en ocho GPU, una pequeña fracción de los costos de entrenamiento de los mejores modelos de la literatura. Demostramos que el Transformer se generaliza bien a otras tareas aplicándolo con éxito al análisis sintáctico de constituyentes en inglés, tanto con datos de entrenamiento grandes como limitados.

# Resumen

Este artículo presenta el Transformer, una nueva arquitectura de red neuronal basada exclusivamente en mecanismos de atención, que supera a los modelos de traducción automática basados en redes recurrentes y convolucionales, logrando un nuevo estado del arte en las tareas de traducción de inglés a alemán y de inglés a francés, con un coste de entrenamiento significativamente menor.

## Planteamiento del Problema

El documento aborda el problema de los modelos de transducción de secuencias, que tradicionalmente se basan en redes neuronales recurrentes o convolucionales complejas. Estos modelos, aunque efectivos, presentan limitaciones en cuanto a la paralelización y el tiempo de entrenamiento, especialmente en secuencias largas. El documento propone una nueva arquitectura de red neuronal, el Transformer, que elimina la necesidad de recurrencia y convoluciones, basándose exclusivamente en mecanismos de atención. El objetivo es mejorar la calidad de la traducción automática, reducir el tiempo de entrenamiento y permitir una mayor paralelización.

## Metodología

El documento propone una nueva arquitectura de red neuronal llamada Transformer, la cual se basa exclusivamente en mecanismos de atención, prescindiendo de recurrencias y convoluciones. El modelo Transformer utiliza atención multi-cabeza y capas *feed-forward*  para modelar las dependencias entre las secuencias de entrada y salida. La arquitectura se compone de un codificador y un decodificador, ambos formados por pilas de capas idénticas. Cada capa del codificador contiene una subcapa de auto-atención multi-cabeza y una subcapa *feed-forward*, mientras que el decodificador incluye una subcapa adicional de atención multi-cabeza sobre la salida del codificador.

La atención se calcula como una suma ponderada de valores, donde los pesos se obtienen mediante una función de compatibilidad entre la consulta y las claves. Se utiliza la atención de producto escalar escalado, que se calcula como:

$ \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V $

donde $Q$ es la matriz de consultas, $K$ es la matriz de claves, $V$ es la matriz de valores, y $d_k$ es la dimensión de las claves.

La atención multi-cabeza se calcula como:

$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$

donde $\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$, $W_i^Q$, $W_i^K$, $W_i^V$ son matrices de proyección lineal, y $W^O$ es otra matriz de proyección.

El modelo también incorpora codificación posicional para inyectar información sobre el orden de los tokens en la secuencia, ya que la atención no tiene en cuenta el orden por sí sola. Se utilizan funciones seno y coseno de diferentes frecuencias para generar los códigos posicionales.

Finalmente, se utilizan conexiones residuales y normalización de capas para mejorar el entrenamiento y la estabilidad del modelo.

## Novedad

El documento presenta una nueva arquitectura de red neuronal llamada Transformer, que se basa exclusivamente en mecanismos de atención, eliminando la necesidad de recurrencia y convoluciones. Los experimentos en tareas de traducción automática muestran que el Transformer es superior en calidad, más paralelizable y requiere significativamente menos tiempo de entrenamiento. El modelo alcanza un BLEU de 28.4 en la tarea de traducción inglés-alemán WMT 2014, superando los resultados existentes, y un BLEU de 41.8 en la tarea de traducción inglés-francés WMT 2014, estableciendo un nuevo estado del arte. Además, el Transformer generaliza bien a otras tareas, como el análisis sintáctico del inglés.

# Estructura del Documento

- 1 Introduction
- 2 Background
- 3 Model Architecture
  - 3.1 Encoder and Decoder Stacks
  - 3.2 Attention
    - 3.2.1 Scaled Dot-Product Attention
    - 3.2.2 Multi-Head Attention
    - 3.2.3 Applications of Attention in our Model
  - 3.3 Position-wise Feed-Forward Networks
  - 3.4 Embeddings and Softmax
  - 3.5 Positional Encoding
- 4 Why Self-Attention
- 5 Training
  - 5.1 Training Data and Batching
  - 5.2 Hardware and Schedule
  - 5.3 Optimizer
  - 5.4 Regularization
- 6 Results
  - 6.1 Machine Translation
  - 6.2 Model Variations
  - 6.3 English Constituency Parsing
- 7 Conclusion
- Acknowledgements
- References
- Attention Visualizations

## 1 Introduction

Las redes neuronales recurrentes, en particular las de memoria a corto plazo (LSTM) y las redes neuronales recurrentes con compuertas (GRU), se han consolidado como el estado del arte en el modelado de secuencias y problemas de transducción, como el modelado del lenguaje y la traducción automática. Los modelos recurrentes típicamente factorizan la computación a lo largo de las posiciones de los símbolos de las secuencias de entrada y salida, generando una secuencia de estados ocultos $h_t$ en función del estado oculto anterior $h_{t-1}$ y la entrada para la posición $t$. Esta naturaleza inherentemente secuencial impide la paralelización dentro de los ejemplos de entrenamiento, lo que se vuelve crítico en longitudes de secuencia más largas. Aunque se han logrado mejoras significativas en la eficiencia computacional a través de trucos de factorización y computación condicional, la limitación fundamental de la computación secuencial persiste. Los mecanismos de atención se han convertido en una parte integral de los modelos de modelado y transducción de secuencias, permitiendo modelar dependencias sin tener en cuenta su distancia en las secuencias de entrada o salida. En este trabajo, se propone el Transformer, una arquitectura de modelo que evita la recurrencia y, en cambio, se basa completamente en un mecanismo de atención para establecer dependencias globales entre la entrada y la salida. El Transformer permite una paralelización significativamente mayor y puede alcanzar un nuevo estado del arte en la calidad de la traducción después de ser entrenado durante tan solo doce horas en ocho GPUs P100.

## 2 Background

La sección 2, "Antecedentes", aborda la problemática de la computación secuencial en modelos de transducción, destacando la necesidad de reducirla. Se mencionan arquitecturas como la GPU Neuronal Extendida, ByteNet y ConvS2S, que utilizan redes convolucionales para procesar las secuencias en paralelo. Estas arquitecturas, sin embargo, requieren un número de operaciones que crece con la distancia entre posiciones (linealmente en ConvS2S y logarítmicamente en ByteNet), dificultando el aprendizaje de dependencias lejanas. El Transformer reduce esta complejidad a un número constante de operaciones, aunque a costa de una menor resolución efectiva debido al promedio de posiciones ponderadas por atención. Este efecto se mitiga con la atención multi-cabeza.

Además, se introduce el concepto de auto-atención, que relaciona diferentes posiciones de una misma secuencia para calcular su representación, y se menciona su uso en diversas tareas. Se señalan las redes de memoria de extremo a extremo, que utilizan un mecanismo de atención recurrente en lugar de la recurrencia alineada a la secuencia. Finalmente, se establece que el Transformer es el primer modelo de transducción que se basa exclusivamente en la auto-atención para computar las representaciones de entrada y salida, sin utilizar redes neuronales recurrentes o convoluciones.

## 3 Model Architecture

El modelo Transformer adopta una arquitectura encoder-decoder. El encoder mapea una secuencia de representaciones de símbolos de entrada $(x_1, ..., x_n)$ a una secuencia de representaciones continuas $z = (z_1, ..., z_n)$. El decoder, dado $z$, genera una secuencia de símbolos de salida $(y_1, ..., y_m)$ un elemento a la vez, de forma auto-regresiva.

El **encoder** consiste en una pila de $N=6$ capas idénticas, cada una con dos sub-capas: un mecanismo de auto-atención multi-cabeza y una red feed-forward totalmente conectada. Se emplean conexiones residuales alrededor de cada sub-capa, seguidas de normalización de capa. Todas las sub-capas y las capas de embedding producen salidas de dimensión $d_{model} = 512$.

El **decoder** también consiste en una pila de $N=6$ capas idénticas. Además de las dos sub-capas del encoder, el decoder inserta una tercera sub-capa que realiza atención multi-cabeza sobre la salida del encoder. Similar al encoder, se usan conexiones residuales y normalización de capa. La sub-capa de auto-atención del decoder se modifica para evitar que las posiciones atiendan a las posiciones subsiguientes, asegurando que las predicciones para la posición $i$ dependan solo de las salidas conocidas en posiciones menores a $i$.

La **atención** se describe como el mapeo de una query y un conjunto de pares clave-valor a una salida, donde todos son vectores. La salida es una suma ponderada de los valores, donde los pesos se calculan mediante una función de compatibilidad entre la query y las claves.

La **atención escalada por producto punto** calcula los productos punto de la query con todas las claves, dividiéndolos por $\sqrt{d_k}$ y aplicando una función softmax para obtener los pesos de los valores:

$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

La **atención multi-cabeza** proyecta linealmente las queries, claves y valores $h$ veces a dimensiones $d_k$, $d_k$ y $d_v$ respectivamente. Luego, realiza la función de atención en paralelo y concatena las salidas, proyectándolas a la dimensión final.

$MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O$

donde $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$

El Transformer usa atención multi-cabeza de tres maneras: en las capas "encoder-decoder attention", donde las queries vienen del decoder y las claves y valores del encoder; en las capas de auto-atención del encoder, donde todos vienen de la capa anterior del encoder; y en las capas de auto-atención del decoder, donde todos vienen de la capa anterior del decoder, con un enmascaramiento para preservar la propiedad auto-regresiva.

Las **redes feed-forward por posición** se aplican a cada posición de manera separada e idéntica, consistiendo en dos transformaciones lineales con una activación ReLU en medio:

$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2$

Las **embeddings y softmax** convierten tokens de entrada y salida a vectores de dimensión $d_{model}$. Se comparte la matriz de pesos entre las capas de embedding y la transformación lineal pre-softmax, multiplicando los pesos por $\sqrt{d_{model}}$ en las capas de embedding.

Los **encodings posicionales** se suman a las embeddings de entrada para inyectar información sobre el orden de la secuencia, usando funciones seno y coseno de diferentes frecuencias:

$PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$

$PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$

## 4 Why Self-Attention

En esta sección se comparan las capas de auto-atención con las capas recurrentes y convolucionales, comúnmente usadas para mapear una secuencia de representaciones de símbolos de longitud variable $(x_1,...,x_n)$ a otra secuencia de igual longitud $(z_1,...,z_n)$, con $x_i, z_i \in \mathbb{R}^d$. Se consideran tres criterios para motivar el uso de la auto-atención: la complejidad computacional total por capa, la cantidad de computación que puede ser paralelizada (medida por el número mínimo de operaciones secuenciales requeridas), y la longitud de la ruta entre dependencias de largo alcance en la red. Se argumenta que las capas de auto-atención conectan todas las posiciones con un número constante de operaciones ejecutadas secuencialmente, mientras que una capa recurrente requiere $O(n)$ operaciones secuenciales. En términos de complejidad computacional, las capas de auto-atención son más rápidas que las capas recurrentes cuando la longitud de la secuencia $n$ es menor que la dimensionalidad de la representación $d$. Se compara la longitud máxima de la ruta entre cualquier par de posiciones de entrada y salida en redes compuestas por diferentes tipos de capas.
Las capas convolucionales, por otro lado, requieren un apilamiento de $O(n/k)$ capas para conectar todas las posiciones, donde $k$ es el tamaño del kernel, lo que aumenta la longitud de la ruta. Se menciona que las capas convolucionales son generalmente más costosas que las capas recurrentes por un factor de $k$. Se introduce la idea de convoluciones separables, que reducen la complejidad, pero aun así resultan en una complejidad similar a la combinación de una capa de auto-atención y una capa feed-forward.
Finalmente, se señala que la auto-atención podría generar modelos más interpretables, observando que las distribuciones de atención parecen exhibir comportamientos relacionados con la estructura sintáctica y semántica de las oraciones.

## 5 Training

En la sección 5, se describe el régimen de entrenamiento de los modelos.

**5.1 Datos de Entrenamiento y Batching:** Se entrenó el modelo en el conjunto de datos estándar WMT 2014 Inglés-Alemán, que consta de aproximadamente 4.5 millones de pares de oraciones. Las oraciones se codificaron utilizando la codificación de pares de bytes, con un vocabulario compartido de origen y destino de aproximadamente 37,000 tokens. Para Inglés-Francés, se utilizó el conjunto de datos WMT 2014 Inglés-Francés, que consta de 36 millones de oraciones y se dividieron los tokens en un vocabulario de 32,000 word-pieces. Los pares de oraciones se agruparon por longitud aproximada de la secuencia, con cada batch de entrenamiento conteniendo aproximadamente 25,000 tokens de origen y 25,000 tokens de destino.

**5.2 Hardware y Programación:** Los modelos se entrenaron en una máquina con 8 GPUs NVIDIA P100. Para los modelos base, cada paso de entrenamiento tomó alrededor de 0.4 segundos, y se entrenaron durante 100,000 pasos (12 horas). Para los modelos grandes, el tiempo de paso fue de 1.0 segundos y se entrenaron durante 300,000 pasos (3.5 días).

**5.3 Optimizador:** Se utilizó el optimizador Adam con $\beta_1 = 0.9$, $\beta_2 = 0.98$ y $\epsilon = 10^{-9}$. La tasa de aprendizaje varió durante el entrenamiento según la fórmula:
$$lrate = d_{model}^{-0.5} \cdot min(step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5})$$
La tasa de aprendizaje aumenta linealmente durante los primeros `warmup_steps` pasos y luego disminuye proporcionalmente a la raíz cuadrada inversa del número de pasos. Se utilizó `warmup_steps = 4000`.

**5.4 Regularización:** Se emplearon tres tipos de regularización durante el entrenamiento:
*   **Dropout Residual:** Se aplicó dropout a la salida de cada subcapa, antes de sumarla a la entrada y normalizarla. También se aplicó dropout a las sumas de las incrustaciones y las codificaciones posicionales. Para el modelo base, se utilizó una tasa de dropout de $p_{drop} = 0.1$.
*   **Label Smoothing:** Se empleó label smoothing con un valor de $\epsilon_{ls} = 0.1$. Esto reduce la perplexidad, pero mejora la precisión y el puntaje BLEU.

## 6 Results

## 6. Resultados

### 6.1 Traducción Automática

En la tarea de traducción de inglés a alemán WMT 2014, el modelo Transformer grande supera a los mejores modelos reportados previamente (incluidos los conjuntos), estableciendo un nuevo puntaje BLEU de 28.4. El entrenamiento tomó 3.5 días en 8 GPUs P100. Incluso el modelo base supera todos los modelos y conjuntos publicados anteriormente, a una fracción del costo de entrenamiento.

En la tarea de traducción de inglés a francés WMT 2014, el modelo grande logra un puntaje BLEU de 41.0, superando a todos los modelos individuales publicados anteriormente, a menos de 1/4 del costo de entrenamiento del modelo de última generación anterior.

Para los modelos base, se utilizó un modelo único obtenido promediando los últimos 5 puntos de control. Para los modelos grandes, se promediaron los últimos 20 puntos de control. Se utilizó búsqueda de haz con un tamaño de haz de 4 y una penalización de longitud $α = 0.6$.

La Tabla 2 resume los resultados y compara la calidad de la traducción y los costos de entrenamiento con otras arquitecturas de modelos.

### 6.2 Variaciones del Modelo

Para evaluar la importancia de los diferentes componentes del Transformer, se varió el modelo base de diferentes maneras, midiendo el cambio en el rendimiento en la traducción de inglés a alemán en el conjunto de desarrollo *newstest2013*.

En la Tabla 3, filas (A), se varía el número de cabezas de atención y las dimensiones de las claves y valores de atención, manteniendo constante la cantidad de cálculo. La atención de una sola cabeza es 0.9 BLEU peor que la mejor configuración.

En la Tabla 3, filas (B), se observa que reducir el tamaño de la clave de atención $d_k$ perjudica la calidad del modelo. Esto sugiere que determinar la compatibilidad no es fácil.

En la Tabla 3, filas (C) y (D), se observa que, como se esperaba, los modelos más grandes son mejores, y el *dropout* es muy útil para evitar el sobreajuste. En la fila (E), se reemplaza la codificación posicional sinusoidal con codificaciones posicionales aprendidas, y se observan resultados casi idénticos al modelo base.

### 6.3 Análisis de la Estructura Sintáctica del Inglés

Para evaluar si el Transformer puede generalizar a otras tareas, se realizaron experimentos en el análisis de la estructura sintáctica del inglés. Se entrenó un Transformer de 4 capas con $d_{model} = 1024$ en la sección del *Wall Street Journal* (WSJ) del *Penn Treebank*, alrededor de 40K oraciones de entrenamiento. También se entrenó en un entorno semi-supervisado, utilizando los corpus más grandes de alta confianza y *BerkleyParser*, con aproximadamente 17M de oraciones. Se utilizó un vocabulario de 16K tokens para la configuración solo WSJ y un vocabulario de 32K tokens para la configuración semi-supervisada.

Los resultados en la Tabla 4 muestran que, a pesar de la falta de ajuste específico de la tarea, el modelo funciona sorprendentemente bien, obteniendo mejores resultados que todos los modelos reportados anteriormente con la excepción de la *Recurrent Neural Network Grammar*. El Transformer supera al *Berkeley-Parser* incluso cuando se entrena solo en el conjunto de entrenamiento WSJ de 40K oraciones.

## 7 Conclusion

En este trabajo, se presentó el Transformer, el primer modelo de transducción de secuencias basado completamente en atención, reemplazando las capas recurrentes más utilizadas en arquitecturas codificador-decodificador con auto-atención multicabezal.

Para tareas de traducción, el Transformer puede entrenarse significativamente más rápido que las arquitecturas basadas en capas recurrentes o convolucionales. En ambas tareas de traducción del WMT 2014, Inglés-Alemán e Inglés-Francés, se alcanzó un nuevo estado del arte. En la primera tarea, el mejor modelo supera incluso a todos los conjuntos informados previamente.

Existe entusiasmo sobre el futuro de los modelos basados en atención y se planea aplicarlos a otras tareas. Se planea extender el Transformer a problemas que involucren modalidades de entrada y salida distintas al texto e investigar mecanismos de atención local y restringida para manejar eficientemente entradas y salidas grandes como imágenes, audio y video. Hacer que la generación sea menos secuencial es otro objetivo de investigación.

## Acknowledgements

Se agradece a Nal Kalchbrenner y Stephan Gouws por sus fructíferos comentarios, correcciones e inspiración.

## References

La sección de Referencias presenta un listado de trabajos académicos que sustentan y complementan la investigación presentada en el artículo. Estos trabajos abarcan una variedad de temas relacionados con el procesamiento del lenguaje natural, el aprendizaje automático y las redes neuronales, incluyendo:

*   **Redes Neuronales Recurrentes (RNN) y sus variantes:**  Se citan trabajos sobre la memoria a corto plazo (LSTM) y las redes neuronales recurrentes con compuertas (GRU), así como sus aplicaciones en el modelado de secuencias y la traducción automática.
*   **Mecanismos de Atención:** Se incluyen referencias a trabajos que introducen y exploran los mecanismos de atención en el contexto de la traducción automática y el modelado de secuencias.
*   **Modelos de Traducción Automática:** Se mencionan trabajos sobre modelos de traducción automática basados en codificadores-decodificadores, tanto recurrentes como convolucionales, así como modelos que utilizan el aprendizaje por refuerzo.
*   **Arquitecturas de Redes Neuronales:** Se citan trabajos sobre arquitecturas como las redes neuronales convolucionales (CNN), las redes con conexiones residuales y las redes con normalización de capas.
*   **Técnicas de Regularización:** Se hace referencia a trabajos sobre técnicas como el dropout y el suavizado de etiquetas (label smoothing), utilizadas para mejorar el entrenamiento de modelos.
*   **Aplicaciones del Procesamiento del Lenguaje Natural:** Se incluyen trabajos sobre diversas aplicaciones como el análisis sintáctico, la comprensión lectora y la generación de texto.
*   **Optimización:** Se cita el algoritmo Adam como método de optimización utilizado en el entrenamiento de los modelos.

En resumen, las referencias proporcionan un contexto sólido y detallado para la investigación presentada, mostrando el estado del arte en el campo y la base sobre la cual se construye el nuevo modelo propuesto.

## Attention Visualizations

La sección "Visualizaciones de Atención" presenta ejemplos de cómo el mecanismo de atención del modelo Transformer aprende relaciones entre palabras en una oración. En la Figura 3, se muestra cómo múltiples cabezas de atención en la capa 5 del codificador atienden a la dependencia distante del verbo "making", completando la frase "making...more difficult". Las diferentes cabezas de atención se representan con distintos colores. La Figura 4 ilustra cómo dos cabezas de atención en la misma capa están involucradas en la resolución de anáfora. La parte superior muestra las atenciones completas, mientras que la parte inferior aísla las atenciones de la palabra "its". La Figura 5 muestra cómo diferentes cabezas de atención aprenden a realizar diferentes tareas, evidenciando comportamientos relacionados con la estructura de la oración.
